{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.7 (default, Mar 23 2020, 22:36:06) \n",
      "[GCC 7.3.0]\n",
      "\n",
      "Pandas 1.0.3\n",
      "Tensorflow 2.2.0\n",
      "Keras 2.3.0-tf\n",
      "\n",
      "tf accessable GPU found: device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# import helper_functions.config as config\n",
    "import helper_functions.data_functions as data_functions\n",
    "#import helper_functions.plotting_functions as plotting_functions\n",
    "\n",
    "# Note: tf 2.1.0 give warning about model weight format when\n",
    "# using class weights. This is the only way to silence without\n",
    "# updating\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "warnings.warn = warn\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "rn.seed(42)\n",
    "os.environ['PYTHONHASHSEED']=str(42)\n",
    "\n",
    "print(f\"Python {sys.version}\")\n",
    "print()\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Tensorflow {tf.__version__}\")\n",
    "print(f\"Keras {keras.__version__}\")\n",
    "print()\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "\n",
    "if 'GPU' in ('').join(str(devices)):\n",
    "    print(\"tf accessable GPU found: \"+devices[-2].physical_device_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, hyperparameters, metrics, data):\n",
    "        self.recurrent_unit_type = hyperparameters['recurrent_unit_type']\n",
    "        self.statefullness = hyperparameters['statefullness']\n",
    "        self.units = hyperparameters['units']\n",
    "        self.learning_rate = hyperparameters['learning_rate']\n",
    "        self.batch_size = hyperparameters['batch_size']\n",
    "        self.past_history = hyperparameters['past_history']\n",
    "        self.dropout = hyperparameters['dropout']\n",
    "        self.class_weight = hyperparameters['class_weight']\n",
    "        self.output_bias = hyperparameters['initial_output_bias']\n",
    "        self.future_target = hyperparameters['future_target']\n",
    "        self.step = hyperparameters['step']\n",
    "        self.test_data_frac = hyperparameters['test_data_frac']\n",
    "        self.validation_data_frac = hyperparameters['validation_data_frac']\n",
    "        self.training_epochs = hyperparameters['training_epochs']\n",
    "        self.verbose = hyperparameters['verbose']\n",
    "            \n",
    "        self.metrics = metrics\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "    def prep_data(self):\n",
    "        # sort dataframe by date\n",
    "        self.data = self.data.sort_values(by='date')\n",
    "        \n",
    "        # split data up into training, testing and validation sets\n",
    "        num_observations = len(self.data)\n",
    "        \n",
    "        test_data = data.tail(int(len(self.data) * self.test_data_frac))\n",
    "        leftover_data = data.head(int(len(self.data) * (1 - self.test_data_frac)))\n",
    "        \n",
    "        validation_data = data.tail(int(len(leftover_data) * self.validation_data_frac))\n",
    "        training_data = data.head(int(len(leftover_data) * (1 - self.validation_data_frac)))\n",
    "        \n",
    "        self.steps_per_epoch = int(len(training_data) // self.batch_size)\n",
    "        self.validation_steps = int(len(validation_data) // self.batch_size)\n",
    "\n",
    "        # create training and vaildation data \n",
    "        x_train, y_train = data_functions.multivariate_data(\n",
    "            training_data,\n",
    "            self.past_history,\n",
    "            self.future_target, \n",
    "            self.step\n",
    "        )\n",
    "\n",
    "        x_validation, y_validation = data_functions.multivariate_data(\n",
    "            validation_data,\n",
    "            self.past_history,\n",
    "            self.future_target, \n",
    "            self.step\n",
    "        )\n",
    "        \n",
    "        x_test, y_test = data_functions.multivariate_data(\n",
    "            test_data,\n",
    "            self.past_history,\n",
    "            self.future_target, \n",
    "            self.step\n",
    "        )\n",
    "\n",
    "        # trim datasets so they are whole number multiples of the batch\n",
    "        # size (needed for statefull LSTM)\n",
    "        start_index = (x_train.shape[0] - (x_train.shape[0] % self.batch_size))\n",
    "        end_index = x_train.shape[0]\n",
    "\n",
    "        self.x_train = np.delete(x_train, range(start_index, end_index), axis=0)\n",
    "        self.y_train = np.delete(y_train, range(start_index, end_index), axis=0)\n",
    "\n",
    "        start_index = (x_validation.shape[0] - (x_validation.shape[0] % self.batch_size))\n",
    "        end_index = x_validation.shape[0]\n",
    "\n",
    "        self.x_validation = np.delete(x_validation, range(start_index, end_index), axis=0)\n",
    "        self.y_validation = np.delete(y_validation, range(start_index, end_index), axis=0)\n",
    "        \n",
    "        start_index = (x_test.shape[0] - (x_test.shape[0] % self.batch_size))\n",
    "        end_index = x_test.shape[0]\n",
    "\n",
    "        self.x_test = np.delete(x_test, range(start_index, end_index), axis=0)\n",
    "        self.y_test = np.delete(y_test, range(start_index, end_index), axis=0)\n",
    "\n",
    "        input_dim = self.x_train.shape[-2:]\n",
    "        self.input_shape = (self.batch_size, input_dim[0], input_dim[1])\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        model = tf.keras.models.Sequential()\n",
    "\n",
    "        if self.recurrent_unit_type == 'LSTM':\n",
    "            model.add(tf.keras.layers.LSTM(\n",
    "                self.units,\n",
    "                batch_input_shape = self.input_shape,\n",
    "                stateful = self.statefullness,\n",
    "                dropout = self.dropout\n",
    "            ))\n",
    "            \n",
    "        elif self.recurrent_unit_type == 'GRU':\n",
    "            model.add(tf.keras.layers.GRU(\n",
    "                self.units,\n",
    "                stateful = self.statefullness,\n",
    "                dropout = self.dropout\n",
    "            ))\n",
    "            \n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = self.output_bias)\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer = tf.keras.optimizers.Adam(lr = self.learning_rate), \n",
    "            loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics = self.metrics\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def train_model(self):\n",
    "        history = self.model.fit(\n",
    "            self.x_train,\n",
    "            self.y_train,\n",
    "            epochs = self.training_epochs,\n",
    "            batch_size = self.batch_size,\n",
    "            steps_per_epoch = self.steps_per_epoch,\n",
    "            validation_data = (self.x_validation, self.y_validation),\n",
    "            validation_steps = self.validation_steps,\n",
    "            class_weight = self.class_weight,\n",
    "            verbose = self.verbose\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        results = self.model.evaluate(\n",
    "            self.x_test,\n",
    "            self.y_test,\n",
    "            batch_size = self.batch_size,\n",
    "            verbose = self.verbose\n",
    "        )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'true_positives': keras.metrics.TruePositives(name='true_positives'),\n",
    "    'false_positives': keras.metrics.FalsePositives(name='false_positives'),\n",
    "    'true_negatives': keras.metrics.TrueNegatives(name='true_negatives'),\n",
    "    'false_negatives': keras.metrics.FalseNegatives(name='false_negatives'), \n",
    "    'AUC': keras.metrics.AUC(name='AUC'),\n",
    "    'matthews_correlation': data_functions.matthews_correlation,\n",
    "    'F1': data_functions.f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8766 entries, 0 to 8765\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   apcp               8766 non-null   float32       \n",
      " 1   crain              8766 non-null   int32         \n",
      " 2   ignition           8766 non-null   int32         \n",
      " 3   mean_air_2m        8766 non-null   float32       \n",
      " 4   mean_rhum_2m       8766 non-null   float32       \n",
      " 5   mean_dpt_2m        8766 non-null   float32       \n",
      " 6   mean_pres_sfc      8766 non-null   float32       \n",
      " 7   mean_uwnd_10m      8766 non-null   float32       \n",
      " 8   mean_vwnd_10m      8766 non-null   float32       \n",
      " 9   mean_vis           8766 non-null   float32       \n",
      " 10  mean_cloud_cover   8766 non-null   float32       \n",
      " 11  date               8766 non-null   datetime64[ns]\n",
      " 12  range_air_2m       8766 non-null   float32       \n",
      " 13  range_rhum_2m      8766 non-null   float32       \n",
      " 14  range_dpt_2m       8766 non-null   float32       \n",
      " 15  range_pres_sfc     8766 non-null   float32       \n",
      " 16  range_uwnd_10m     8766 non-null   float32       \n",
      " 17  range_vwnd_10m     8766 non-null   float32       \n",
      " 18  range_vis          8766 non-null   float32       \n",
      " 19  range_cloud_cover  8766 non-null   float32       \n",
      "dtypes: datetime64[ns](1), float32(17), int32(2)\n",
      "memory usage: 719.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_file = '/mnt/SSD/data/training_data/1992-2015_california_standard_scaled_mean_range_onehot_month_selective_box_cox.parquet'\n",
    "raw_data = pd.read_parquet(data_file)\n",
    "\n",
    "# Pick one spatial bin with fires\n",
    "data = raw_data[(raw_data['raw_lat'] == 39.42233) & (raw_data['raw_lon'] == -120.6546)]\n",
    "\n",
    "# Also drop unnecessary columns\n",
    "data.drop([\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'raw_lat',\n",
    "    'raw_lon',\n",
    "    'total_fires',\n",
    "    'veg',\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'Septermber',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December'], axis=1, inplace=True)\n",
    "\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# calculate class weights and initial output bias from ignition frequency in the data \n",
    "class_weight = data_functions.get_class_weights(data)\n",
    "output_bias = data_functions.get_initial_output_bias(data)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics to monitor during the run\n",
    "\n",
    "metrics = {\n",
    "    'true_positives': keras.metrics.TruePositives(name='true_positives'),\n",
    "    'false_positives': keras.metrics.FalsePositives(name='false_positives'),\n",
    "    'true_negatives': keras.metrics.TrueNegatives(name='true_negatives'),\n",
    "    'false_negatives': keras.metrics.FalseNegatives(name='false_negatives'), \n",
    "    'AUC': keras.metrics.AUC(name='AUC'),\n",
    "    'matthews_correlation': data_functions.matthews_correlation,\n",
    "    'F1': data_functions.f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set run hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "    'recurrent_unit_type': 'LSTM',\n",
    "    'statefullness': False,\n",
    "    'units': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 128,\n",
    "    'past_history': 8,\n",
    "    'dropout': 0.03,\n",
    "    'class_weight': class_weight,\n",
    "    'initial_output_bias': output_bias,\n",
    "    'future_target': 1,\n",
    "    'step': 1,\n",
    "    'test_data_frac': 0.25,\n",
    "    'validation_data_frac': 0.5,\n",
    "    'training_epochs': 25,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# set parameters to vary \n",
    "\n",
    "experimental_parameters = ['statefullness', 'RNN_type', 'past_history']\n",
    "\n",
    "parameter_sets = {\n",
    "    'Stateless LSTM, past history 4': [False, 'LSTM', 4],\n",
    "    'Stateless LSTM, past history 8': [False, 'LSTM', 8],\n",
    "    'Stateless LSTM, past history 16': [False, 'LSTM', 16],\n",
    "    'Stateless LSTM, past history 32': [False, 'LSTM', 32],\n",
    "    'Statefull LSTM, past history 4': [True, 'LSTM', 4],\n",
    "    'Statefull LSTM, past history 8': [True, 'LSTM', 8],\n",
    "    'Statefull LSTM, past history 16': [True, 'LSTM', 16],\n",
    "    'Statefull LSTM, past history 32': [True, 'LSTM', 32],\n",
    "    'Stateless GRU, past history 4': [False, 'GRU', 4],\n",
    "    'Stateless GRU, past history 8': [False, 'GRU', 8],\n",
    "    'Stateless GRU, past history 16': [False, 'GRU', 16],\n",
    "    'Stateless GRU, past history 32': [False, 'GRU', 32],\n",
    "    'Statefull GRU, past history 4': [True, 'GRU', 4],\n",
    "    'Statefull GRU, past history 8': [True, 'GRU', 8],\n",
    "    'Statefull GRU, past history 16': [True, 'GRU', 16],\n",
    "    'Statefull GRU, past history 32': [True, 'GRU', 32]\n",
    "}\n",
    "\n",
    "\n",
    "# set up pandas dataframe to hold results\n",
    "\n",
    "metric_names = ['loss'] + list(metrics.keys())\n",
    "metric_mean_names = []\n",
    "metric_std_names = []\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    metric_mean_names.append(metric_name)\n",
    "    metric_std_names.append(f'{metric_name}_stdev')\n",
    "    \n",
    "column_names = ['description', 'peak_memory', 'peak_memory_stdev'] + experimental_parameters + metric_mean_names + metric_std_names\n",
    "results = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stateless LSTM, past history 4, run 0, 48 remaining\n",
      "Stateless LSTM, past history 4, run 1, 47 remaining\n",
      "Stateless LSTM, past history 4, run 2, 46 remaining\n",
      "Stateless LSTM, past history 8, run 0, 45 remaining\n",
      "Stateless LSTM, past history 8, run 1, 44 remaining\n",
      "Stateless LSTM, past history 8, run 2, 43 remaining\n"
     ]
    }
   ],
   "source": [
    "# Do training runs on experimental conditions\n",
    "\n",
    "num_repititions = 3\n",
    "total_training_runs = num_repititions * len(parameter_sets)\n",
    "\n",
    "for description, parameter_set in parameter_sets.items():\n",
    "    run_results = []\n",
    "    memory_use = []\n",
    "    \n",
    "    for i in range(num_repititions):\n",
    "        print(f'{description}, run {i}, {total_training_runs} remaining')\n",
    "\n",
    "        LSTM_RNN = NeuralNet(hyperparameters, list(metrics.values()), data)\n",
    "\n",
    "        LSTM_RNN.statefullness = parameter_set[0]\n",
    "        LSTM_RNN.recurrent_unit_type = parameter_set[1]\n",
    "        LSTM_RNN.batch_size = parameter_set[2]\n",
    "\n",
    "        LSTM_RNN.prep_data()\n",
    "        \n",
    "        tracemalloc.start()\n",
    "        LSTM_RNN.build_model()\n",
    "        LSTM_RNN.train_model()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        memory_use.append(peak)\n",
    "\n",
    "        result = LSTM_RNN.evaluate_model()\n",
    "        run_results.append(result)\n",
    "        \n",
    "        total_training_runs -= 1\n",
    "        \n",
    "        del LSTM_RNN.model\n",
    "        K.clear_session()\n",
    "        \n",
    "    run_means = np.array(run_results).mean(axis=0)\n",
    "    run_stdev = np.array(run_results).std(axis=0)\n",
    "    peak_memory_mean = np.array(memory_use).mean()\n",
    "    peak_memory_std = np.array(memory_use).std()\n",
    "    result_row = [description, peak_memory_mean, peak_memory_std] + parameter_set + list(run_means) + list(run_stdev)\n",
    "    results.loc[len(results)] = result_row    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "results['past_history'] = results['past_history'].astype('float64')\n",
    "conditions = results.groupby(['statefullness', 'RNN_type'])\n",
    "\n",
    "plt.subplots(1, 4, figsize=(16,4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "\n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['matthews_correlation'])\n",
    "    y_err = np.array(group['matthews_correlation_stdev'])\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('Matthews Correlation Coef.')\n",
    "    \n",
    "plt.subplot(1, 4, 2)\n",
    "\n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['AUC'])\n",
    "    y_err = np.array(group['AUC_stdev'])\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('Area under the ROC curve')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "    \n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['F1'])\n",
    "    y_err = np.array(group['F1_stdev'])\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('F1 score')\n",
    "    \n",
    "plt.subplot(1, 4, 4)\n",
    "\n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['peak_memory'] / 10**6)\n",
    "    y_err = np.array(group['peak_memory_stdev'] / 10**6)\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('Peak memory use (MB)')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set run hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "    'recurrent_unit_type': 'LSTM',\n",
    "    'statefullness': False,\n",
    "    'units': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 128,\n",
    "    'past_history': 32,\n",
    "    'dropout': 0.03,\n",
    "    'class_weight': class_weight,\n",
    "    'initial_output_bias': output_bias,\n",
    "    'future_target': 1,\n",
    "    'step': 1,\n",
    "    'test_data_frac': 0.25,\n",
    "    'validation_data_frac': 0.5,\n",
    "    'training_epochs': 50,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# set parameters to vary \n",
    "\n",
    "experimental_parameters = ['statefullness','batch_size']\n",
    "\n",
    "parameter_sets = {\n",
    "    'Stateless GRU, batch size 32': [False, 32],\n",
    "    'Stateless GRU, batch size 64': [False, 64],\n",
    "    'Stateless GRU, batch size 128': [False, 128],\n",
    "    'Stateless GRU, batch size 256': [False, 256],\n",
    "    'Stateless GRU, batch size 512': [False, 512],\n",
    "    'Stateless GRU, batch size 1024': [False, 1024],\n",
    "    'Stateless GRU, batch size 2048': [False, 2048],\n",
    "    'Statefull GRU, batch size 32': [True, 32],\n",
    "    'Statefull GRU, batch size 64': [True, 64],\n",
    "    'Statefull GRU, batch size 128': [True, 128],\n",
    "    'Statefull GRU, batch size 256': [True, 256],\n",
    "    'Statefull GRU, batch size 512': [True, 512],\n",
    "    'Statefull GRU, batch size 1024': [True, 1024],\n",
    "    'Statefull GRU, batch size 2048': [True, 2084]\n",
    "}\n",
    "\n",
    "# set up pandas dataframe to hold results\n",
    "\n",
    "metric_names = ['loss'] + list(metrics.keys())\n",
    "metric_mean_names = []\n",
    "metric_std_names = []\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    metric_mean_names.append(metric_name)\n",
    "    metric_std_names.append(f'{metric_name}_stdev')\n",
    "    \n",
    "column_names = ['description', 'peak_memory', 'peak_memory_stdev'] + experimental_parameters + metric_mean_names + metric_std_names\n",
    "results = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training runs on experimental conditions\n",
    "\n",
    "num_repititions = 3\n",
    "total_training_runs = num_repititions * len(parameter_sets)\n",
    "\n",
    "for description, parameter_set in parameter_sets.items():\n",
    "    run_results = []\n",
    "    memory_use = []\n",
    "    \n",
    "    for i in range(num_repititions):\n",
    "        print(f'{description}, run {i}, {total_training_runs} remaining')\n",
    "\n",
    "        GRU_RNN = NeuralNet(hyperparameters, list(metrics.values()), data)\n",
    "\n",
    "        GRU_RNN.statefullness = parameter_set[0]\n",
    "        GRU_RNN.past_history = parameter_set[1]\n",
    "\n",
    "        GRU_RNN.prep_data()\n",
    "        \n",
    "        tracemalloc.start()\n",
    "        GRU_RNN.build_model()\n",
    "        GRU_RNN.train_model()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        memory_use.append(peak)\n",
    "\n",
    "        result = GRU_RNN.evaluate_model()\n",
    "        run_results.append(result)\n",
    "        \n",
    "        total_training_runs -= 1\n",
    "        \n",
    "    run_means = np.array(run_results).mean(axis=0)\n",
    "    run_stdev = np.array(run_results).std(axis=0)\n",
    "    peak_memory_mean = np.array(memory_use).mean()\n",
    "    peak_memory_std = np.array(memory_use).std()\n",
    "    result_row = [description, peak_memory_mean, peak_memory_std] + parameter_set + list(run_means) + list(run_stdev)\n",
    "    results.loc[len(results)] = result_row   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "results['past_history'] = results['past_history'].astype('float64')\n",
    "conditions = results.groupby(['statefullness'])\n",
    "\n",
    "plt.subplots(1, 4, figsize=(16,4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "\n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['matthews_correlation'])\n",
    "    y_err = np.array(group['matthews_correlation_stdev'])\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('Matthews Correlation Coef.')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "\n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['AUC'])\n",
    "    y_err = np.array(group['AUC_stdev'])\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('Area under the ROC curve')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "    \n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['F1'])\n",
    "    y_err = np.array(group['F1_stdev'])\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('F1 score')\n",
    "    \n",
    "plt.subplot(1, 4, 4)\n",
    "\n",
    "for name, group in conditions:\n",
    "    x = np.array(group['past_history'])\n",
    "    y = np.array(group['peak_memory'] / 10**6)\n",
    "    y_err = np.array(group['peak_memory_stdev'] / 10**6)\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "    plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel('Past history (days)')\n",
    "    plt.ylabel('Peak memory use (MB)')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set run hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "    'recurrent_unit_type': 'GRU',\n",
    "    'statefullness': False,\n",
    "    'units': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 1024,\n",
    "    'past_history': 28,\n",
    "    'dropout': 0.03,\n",
    "    'class_weight': class_weight,\n",
    "    'initial_output_bias': output_bias,\n",
    "    'future_target': 1,\n",
    "    'step': 1,\n",
    "    'test_data_frac': 0.25,\n",
    "    'validation_data_frac': 0.5,\n",
    "    'training_epochs': 25,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# set parameters to vary \n",
    "\n",
    "experimental_parameters = ['units']\n",
    "\n",
    "parameter_sets = {\n",
    "    'GRU units: 1': [1],\n",
    "    'GRU units: 2': [2],\n",
    "    'GRU units: 3': [3],\n",
    "    'GRU units: 4': [4],\n",
    "    'GRU units: 5': [5],\n",
    "    'GRU units: 6': [6],\n",
    "    'GRU units: 7': [7],\n",
    "    'GRU units: 8': [8],\n",
    "    'GRU units: 9': [9],\n",
    "    'GRU units: 10': [10]\n",
    "}\n",
    "\n",
    "# set up pandas dataframe to hold results\n",
    "\n",
    "metric_names = ['loss'] + list(metrics.keys())\n",
    "metric_mean_names = []\n",
    "metric_std_names = []\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    metric_mean_names.append(metric_name)\n",
    "    metric_std_names.append(f'{metric_name}_stdev')\n",
    "    \n",
    "column_names = ['description', 'peak_memory', 'peak_memory_stdev'] + experimental_parameters + metric_mean_names + metric_std_names\n",
    "results = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training runs on experimental conditions\n",
    "\n",
    "num_repititions = 10\n",
    "total_training_runs = num_repititions * len(parameter_sets)\n",
    "\n",
    "for description, parameter_set in parameter_sets.items():\n",
    "    run_results = []\n",
    "    memory_use = []\n",
    "    \n",
    "    for i in range(num_repititions):\n",
    "        print(f'{description}, run {i}, {total_training_runs} remaining')\n",
    "\n",
    "        GRU_RNN = NeuralNet(hyperparameters, list(metrics.values()), data)\n",
    "\n",
    "        GRU_RNN.units = parameter_set[0]\n",
    "\n",
    "        GRU_RNN.prep_data()\n",
    "        \n",
    "        tracemalloc.start()\n",
    "        GRU_RNN.build_model()\n",
    "        GRU_RNN.train_model()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        memory_use.append(peak)\n",
    "\n",
    "        result = GRU_RNN.evaluate_model()\n",
    "        run_results.append(result)\n",
    "        \n",
    "        total_training_runs -= 1\n",
    "        \n",
    "    run_means = np.array(run_results).mean(axis=0)\n",
    "    run_stdev = np.array(run_results).std(axis=0)\n",
    "    peak_memory_mean = np.array(memory_use).mean()\n",
    "    peak_memory_std = np.array(memory_use).std()\n",
    "    result_row = [description, peak_memory_mean, peak_memory_std] + parameter_set + list(run_means) + list(run_stdev)\n",
    "    results.loc[len(results)] = result_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "results['units'] = results['units'].astype('float64')\n",
    "\n",
    "plt.subplots(1, 4, figsize=(16,4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "\n",
    "x = np.array(results['units'])\n",
    "y = np.array(results['matthews_correlation'])\n",
    "y_err = np.array(results['matthews_correlation_stdev'])\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('GRU units')\n",
    "plt.ylabel('Matthews Correlation Coef.')\n",
    "    \n",
    "plt.subplot(1, 4, 2)\n",
    "\n",
    "x = np.array(results['units'])\n",
    "y = np.array(results['AUC'])\n",
    "y_err = np.array(results['AUC_stdev'])\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('GRU units')\n",
    "plt.ylabel('Area under the ROC curve')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "    \n",
    "x = np.array(results['units'])\n",
    "y = np.array(results['F1'])\n",
    "y_err = np.array(results['F1_stdev'])\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('GRU units')\n",
    "plt.ylabel('F1 score')\n",
    "    \n",
    "plt.subplot(1, 4, 4)\n",
    "\n",
    "x = np.array(results['units'])\n",
    "y = np.array(results['peak_memory'] / 10**6)\n",
    "y_err = np.array(results['peak_memory_stdev'] / 10**6)\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('GRU units')\n",
    "plt.ylabel('Peak memory use (MB)')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set run hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "    'recurrent_unit_type': 'GRU',\n",
    "    'statefullness': False,\n",
    "    'units': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 1024,\n",
    "    'past_history': 28,\n",
    "    'dropout': 0.01,\n",
    "    'class_weight': class_weight,\n",
    "    'initial_output_bias': output_bias,\n",
    "    'future_target': 1,\n",
    "    'step': 1,\n",
    "    'test_data_frac': 0.25,\n",
    "    'validation_data_frac': 0.5,\n",
    "    'training_epochs': 25,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# set parameters to vary \n",
    "\n",
    "experimental_parameters = ['dropout']\n",
    "\n",
    "parameter_sets = {\n",
    "    'Dropout: 0.01': [0.01],\n",
    "    'Dropout: 0.05': [0.05],\n",
    "    'Dropout: 0.10': [0.10],\n",
    "    'Dropout: 0.15': [0.15],\n",
    "    'Dropout: 0.20': [0.20],\n",
    "    'Dropout: 0.25': [0.25],\n",
    "    'Dropout: 0.30': [0.30]\n",
    "}\n",
    "\n",
    "# set up pandas dataframe to hold results\n",
    "\n",
    "metric_names = ['loss'] + list(metrics.keys())\n",
    "metric_mean_names = []\n",
    "metric_std_names = []\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    metric_mean_names.append(metric_name)\n",
    "    metric_std_names.append(f'{metric_name}_stdev')\n",
    "    \n",
    "column_names = ['description', 'peak_memory', 'peak_memory_stdev'] + experimental_parameters + metric_mean_names + metric_std_names\n",
    "results = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training runs on experimental conditions\n",
    "\n",
    "num_repititions = 20\n",
    "total_training_runs = num_repititions * len(parameter_sets)\n",
    "\n",
    "for description, parameter_set in parameter_sets.items():\n",
    "    run_results = []\n",
    "    memory_use = []\n",
    "    \n",
    "    for i in range(num_repititions):\n",
    "        print(f'{description}, run {i}, {total_training_runs} remaining')\n",
    "\n",
    "        GRU_RNN = NeuralNet(hyperparameters, list(metrics.values()), data)\n",
    "        GRU_RNN.dropout = parameter_set[0]\n",
    "        GRU_RNN.prep_data()\n",
    "        \n",
    "        tracemalloc.start()\n",
    "        GRU_RNN.build_model()\n",
    "        GRU_RNN.train_model()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        memory_use.append(peak)\n",
    "\n",
    "        result = GRU_RNN.evaluate_model()\n",
    "        run_results.append(result)\n",
    "        \n",
    "        total_training_runs -= 1\n",
    "        \n",
    "    run_means = np.array(run_results).mean(axis=0)\n",
    "    run_stdev = np.array(run_results).std(axis=0)\n",
    "    peak_memory_mean = np.array(memory_use).mean()\n",
    "    peak_memory_std = np.array(memory_use).std()\n",
    "    result_row = [description, peak_memory_mean, peak_memory_std] + parameter_set + list(run_means) + list(run_stdev)\n",
    "    results.loc[len(results)] = result_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "#results['units'] = results['units'].astype('float64')\n",
    "\n",
    "plt.subplots(1, 4, figsize=(16,4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "\n",
    "x = np.array(results['dropout'])\n",
    "y = np.array(results['matthews_correlation'])\n",
    "y_err = np.array(results['matthews_correlation_stdev'])\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('Matthews Correlation Coef.')\n",
    "    \n",
    "plt.subplot(1, 4, 2)\n",
    "\n",
    "x = np.array(results['dropout'])\n",
    "y = np.array(results['AUC'])\n",
    "y_err = np.array(results['AUC_stdev'])\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('Area under the ROC curve')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "    \n",
    "x = np.array(results['dropout'])\n",
    "y = np.array(results['F1'])\n",
    "y_err = np.array(results['F1_stdev'])\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('F1 score')\n",
    "    \n",
    "plt.subplot(1, 4, 4)\n",
    "\n",
    "x = np.array(results['dropout'])\n",
    "y = np.array(results['peak_memory'] / 10**6)\n",
    "y_err = np.array(results['peak_memory_stdev'] / 10**6)\n",
    "plt.plot(x, y, marker=\"o\", linestyle=\"-\", label=name)\n",
    "plt.fill_between(x, y-y_err, y+y_err, alpha=0.25)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('Peak memory use (MB)')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
