{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM scaling\n",
    "After successfully optimizing a LSTM based neural network to predict wildfire risk in one California geopsatial bin, the next step is scale the model to make predictions for all 410 geospatial bins. A parallel LSTM architecture will be employed. Each of the 410 geospatial bins in California will get it's own input layer and LSTM layer, these will then be concatenated via a merge layer. The model output will be a 410 member vector where each element represents the prediction for one of the original geospatial bins.\n",
    "\n",
    "### Goal: \n",
    "Scale LSTM neural network to make predictions for all 410 geospatial bins in California. Save trained model weights and optimized hyperparameters for deployment.\n",
    "\n",
    "### Plan:\n",
    "1. Prep data from one geospatial bin for input into LSTM\n",
    "2. Build parallel LSTM model\n",
    "3. Using hyperparameters from the single LSTM model as a starting point, tune and evaluate the parallel LSTM model.\n",
    "4. Save trained model weights and hyperparameters for model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c9afcf72859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting_functions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplotting_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_functions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelper_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Python {sys.version}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/springboard/capstone/wildfire_production/notebooks/helper_functions/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m metrics = [\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruePositives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true_positives'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFalsePositives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'false_positives'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrueNegatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true_negatives'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, thresholds, name, dtype)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0mthresholds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, confusion_matrix_cond, thresholds, name, dtype)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;34m'accumulator'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m         initializer=init_ops.zeros_initializer)\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, aggregation, synchronization, initializer, dtype)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0;31m### End: For use by subclasses ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2594\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2597\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1409\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1555\u001b[0m               \u001b[0mshared_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m               \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m               graph_mode=self._in_graph_mode)\n\u001b[0m\u001b[1;32m   1558\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m         if (self._in_graph_mode and initial_value is not None and\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36meager_safe_variable_handle\u001b[0;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[1;32m    230\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m   return _variable_handle_from_shape_and_dtype(\n\u001b[0;32m--> 232\u001b[0;31m       shape, dtype, shared_name, name, graph_mode, initial_value)\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[0;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# support string tensors, we encode the assertion string in the Op name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     gen_logging_ops._assert(  # pylint: disable=protected-access\n\u001b[0;32m--> 164\u001b[0;31m         math_ops.logical_not(exists), [exists], name=\"EagerVariableNameReuse\")\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mhandle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpp_shape_inference_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCppShapeInferenceResult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHandleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_logging_ops.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: tf 2.1.0 give warning about model weight format when\n",
    "# using class weights. This is the only way to silence without\n",
    "# updating\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from pickle import dump\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Note: sklearn forces depreciation warnings\n",
    "# This is the only way to silence them without updating\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import helper_functions.plotting_functions as plotting_functions\n",
    "import helper_functions.data_functions as data_functions\n",
    "import helper_functions.config as config\n",
    "\n",
    "print(f\"Python {sys.version}\")\n",
    "print()\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Tensorflow {tf.__version__}\")\n",
    "print(f\"Keras {keras.__version__}\")\n",
    "print(f\"SciKit Learn {sk.__version__}\")\n",
    "print()\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "\n",
    "if 'GPU' in ('').join(str(devices)):\n",
    "    print(\"tf accessable GPU found: \"+devices[-2].physical_device_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9a4a16ae85bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load configuration variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_data_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures_to_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_to_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# load configuration variables\n",
    "data_file = config.training_data_file\n",
    "\n",
    "features = config.features\n",
    "features_to_scale = config.features_to_scale\n",
    "months = config.months\n",
    "\n",
    "left = config.left\n",
    "right = config.right\n",
    "bottom = config.bottom\n",
    "top = config.top\n",
    "wspace = config.wspace\n",
    "hspace = config.hspace\n",
    "fig_rows = config.fig_rows\n",
    "fig_cols = config.fig_cols\n",
    "plot_height = config.plot_height\n",
    "plot_width = config.plot_width\n",
    "plot_locations = config.plot_locations\n",
    "\n",
    "metrics = config.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use during training \n",
    "features = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'mean_air_2m',\n",
    "    'mean_apcp',\n",
    "    'mean_rhum_2m',\n",
    "    'mean_dpt_2m',\n",
    "    'mean_pres_sfc',\n",
    "    'mean_uwnd_10m',\n",
    "    'mean_vwnd_10m',\n",
    "    'mean_cloud_cover',\n",
    "    'ignition',\n",
    "    'date'\n",
    "]\n",
    "\n",
    "features_to_scale = [\n",
    "    'mean_air_2m',\n",
    "    'mean_apcp',\n",
    "    'mean_rhum_2m',\n",
    "    'mean_dpt_2m',\n",
    "    'mean_pres_sfc',\n",
    "    'mean_uwnd_10m',\n",
    "    'mean_vwnd_10m',\n",
    "    'mean_cloud_cover',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and grab features of intrest\n",
    "raw_data = pd.read_parquet(data_file)\n",
    "data = raw_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fires = data['ignition'].sum()\n",
    "total_observations = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# get months from each row of data, reshape from wide to long\n",
    "month = np.array(pd.DatetimeIndex(data['date']).month).reshape(-1, 1)\n",
    "\n",
    "# onehot encode\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "# convert to pandas dataframe with named columns\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=months)\n",
    "\n",
    "# set type to int\n",
    "onehot_month_df = onehot_month_df.astype('int32')\n",
    "\n",
    "# reset indexes\n",
    "onehot_month_df.reset_index(drop=True, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# join months back to data along rows\n",
    "data = pd.concat([data, onehot_month_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling and normalization\n",
    "Next, we will use the Box-Cox transform to enforce a normal distribution on our data, then use a min-max scaler to put it the range (-1, 1) so that it matches the effective range of the tanh activation function used by the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weather data distributions before scaling\n",
    "xlabels = features_to_scale\n",
    "ylabels = ['Density'] * len(features_to_scale)\n",
    "\n",
    "fig, ax = plt.subplots(fig_rows, fig_cols, figsize=(plot_width, plot_height))\n",
    "fig.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "\n",
    "for i in range(len(features_to_scale)):\n",
    "    ax[plot_locations[i]] = plotting_functions.one_sample_density_plot(\n",
    "        ax,\n",
    "        plot_locations[i], \n",
    "        data, \n",
    "        features_to_scale[i], \n",
    "        features_to_scale[i], \n",
    "        features_to_scale[i], \n",
    "        ylabels[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of the weather variables are somewhat normally distributed and some are far from it. The ranges also vary widely. Lets do what we can to fix that before the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use box-cox to enforce normal distributions on data\n",
    "qt = QuantileTransformer(random_state = 0, output_distribution = 'normal')\n",
    "normalized_data = pd.DataFrame(qt.fit_transform(data[features_to_scale]), columns=features_to_scale)\n",
    "data[features_to_scale] = normalized_data\n",
    "dump(qt, open('./data_transformations/quantile_transformer', 'wb'))\n",
    "\n",
    "# min-max scale data into (-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_features = scaler.fit_transform(data[features_to_scale])\n",
    "data[features_to_scale] = scaled_features\n",
    "dump(scaler, open('./data_transformations/min_max_scaler', 'wb'))\n",
    "\n",
    "# reset dtype\n",
    "data[features_to_scale] = data[features_to_scale].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weather data distributions after scaling\n",
    "xlabels = features_to_scale\n",
    "ylabels = ['Density'] * len(features_to_scale)\n",
    "\n",
    "# Make density plots\n",
    "fig, ax = plt.subplots(fig_rows, fig_cols, figsize=(plot_width, plot_height))\n",
    "fig.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "\n",
    "for i in range(len(features_to_scale)):\n",
    "    ax[plot_locations[i]] = plotting_functions.one_sample_density_plot(\n",
    "        ax,\n",
    "        plot_locations[i], \n",
    "        data, \n",
    "        features_to_scale[i], \n",
    "        features_to_scale[i], \n",
    "        features_to_scale[i], \n",
    "        ylabels[i]\n",
    "    )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for vegetation coverage and categorical rain, the distributions now look much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation and testing sets\n",
    "Note: since temporal order matters here, we cannot simply randomly sample the data - we need to preserve the order. One possible improvement could be to break the data into many smaller ordered chunks and then sample those randomly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure data is sorted by date, then drop date column\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data = data.set_index('date')\n",
    "data = data.sort_index()\n",
    "data.reset_index(inplace=True)#, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data up into training, testing and validation sets\n",
    "testing_data = data.tail(int(len(data) * 0.25))\n",
    "leftover_data = data.head(int(len(data) * 0.75))\n",
    "validation_data = data.tail(int(len(leftover_data) * 0.5))\n",
    "training_data = data.head(int(len(leftover_data) * 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the distributions of each weather variable across the train, validation and test sets just to be sure that our samples are matched to a first approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot split data distributions\n",
    "xlabels = features_to_scale\n",
    "ylabels = ['Density'] * len(features_to_scale)\n",
    "\n",
    "# Make density plots\n",
    "fig, ax = plt.subplots(fig_rows, fig_cols, figsize=(plot_width, plot_height))\n",
    "fig.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "\n",
    "for i in range(len(features_to_scale)):\n",
    "    ax[plot_locations[i]] = plotting_functions.three_sample_density_plot(\n",
    "        ax,\n",
    "        plot_locations[i], \n",
    "        training_data, \n",
    "        validation_data, \n",
    "        test_data, \n",
    "        features_to_scale[i], \n",
    "        features_to_scale[i], \n",
    "        features_to_scale[i], \n",
    "        ylabels[i]\n",
    "    )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data for parallel LSTM\n",
    "This part is a bit tricky - we need to form our data into samples, each sample containing 5 days of past history. Then we need a sequence of samples which spans the data set for each geospatial bin. Therefore, our input data will be a 410 member list of numpy arrays. The labels will be formated such that the first dimension is time and each time step contains a 410 member vector, one element for each of the 410 bins we are making predictions for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fires = data['ignition'].sum()\n",
    "total_observations = len(data)\n",
    "\n",
    "print(\"Fires observed: {}\".format(num_fires))\n",
    "print(\"Total observations: {}\".format(total_observations))\n",
    "print()\n",
    "print(\"Training data: {} observations\".format(len(training_data)))\n",
    "print(\"Validation data: {} observations\".format(len(validation_data)))\n",
    "print(\"Testing data: {} observations\".format(len(testing_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 410 member lists, one element for each geospatial bin (and network input)\n",
    "# each element of the list contains samples with a duration of past_history\n",
    "\n",
    "past_history = 5\n",
    "future_target = 1\n",
    "step = 1\n",
    "\n",
    "x_training, y_training = format_features_labels_for_LSTM(\n",
    "    training_data,        # incomming_data\n",
    "    past_history,         # size of past history time chunk\n",
    "    future_target,        # number of future timepoints to predict from each history time chunk\n",
    "    step,                 # number of timepoints to move the history time chunk as we slide over the data\n",
    ")\n",
    "\n",
    "x_validation, y_validation = format_features_labels_for_LSTM(\n",
    "    validation_data,      # incomming_data\n",
    "    past_history,         # size of past history time chun\n",
    "    future_target,        # number of future timepoints to predict from each history time chunk\n",
    "    step,                 # number of timepoints to move the history time chunk as we slide over the data\n",
    ")\n",
    "\n",
    "x_testing, y_testing = format_features_labels_for_LSTM(\n",
    "    testing_data,         # incomming_data\n",
    "    past_history,         # size of past history time chun\n",
    "    future_target,        # number of future timepoints to predict from each history time chunk\n",
    "    step,                 # number of timepoints to move the history time chunk as we slide over the data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_training is {}, length: {}, member shape: {}.\".format(type(x_training), len(x_training), x_training[0].shape))\n",
    "print(\"y_training is {}, length: {}, member shape: {}.\".format(type(y_training), len(y_training), y_training[0].shape))\n",
    "print()\n",
    "print(\"x_validation is {}, length: {}, member shape: {}.\".format(type(x_validation), len(x_validation), x_validation[0].shape))\n",
    "print(\"y_validation is {}, length: {}, member shape: {}.\".format(type(y_validation), len(y_validation), y_validation[0].shape))\n",
    "print()\n",
    "print(\"x_testing is {}, length: {}, member shape: {}.\".format(type(x_testing), len(x_testing), x_testing[0].shape))\n",
    "print(\"y_testing is {}, length: {}, member shape: {}.\".format(type(y_testing), len(y_testing), y_testing[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training, y_training = trim_and_reshape_for_LSTM(x_training, y_training)\n",
    "x_validation, y_validation = trim_and_reshape_for_LSTM(x_validation, y_validation)\n",
    "x_testing, y_testing = trim_and_reshape_for_LSTM(x_testing, y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_training is {}, length: {}, member shape: {}.\".format(type(x_training), len(x_training), x_training[0].shape))\n",
    "print(\"y_training is {}, length: {}, member shape: {}.\".format(type(y_training), len(y_training), y_training[0].shape))\n",
    "print()\n",
    "print(\"x_validation is {}, length: {}, member shape: {}.\".format(type(x_validation), len(x_validation), x_validation[0].shape))\n",
    "print(\"y_validation is {}, length: {}, member shape: {}.\".format(type(y_validation), len(y_validation), y_validation[0].shape))\n",
    "print()\n",
    "print(\"x_testing is {}, length: {}, member shape: {}.\".format(type(x_testing), len(x_testing), x_testing[0].shape))\n",
    "print(\"y_testing is {}, length: {}, member shape: {}.\".format(type(y_testing), len(y_testing), y_testing[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim each sample again to be a multiple of the batch size so we\n",
    "# can do minibatch gradient descent - Note: this is required for statefull LSTMs\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "for i in range(len(x_training)):\n",
    "    start_index = (x_training[i].shape[0] - (x_training[i].shape[0] % BATCH_SIZE))\n",
    "    end_index = x_training[i].shape[0]\n",
    "\n",
    "    x_training[i] = np.delete(x_training[i], range(start_index, end_index), axis=0)\n",
    "\n",
    "y_training = np.delete(y_training, range(start_index, end_index), axis=0)\n",
    "\n",
    "for i in range(len(x_validation)):\n",
    "    start_index = (x_validation[i].shape[0] - (x_validation[i].shape[0] % BATCH_SIZE))\n",
    "    end_index = x_validation[i].shape[0]\n",
    "\n",
    "    x_validation[i] = np.delete(x_validation[i], range(start_index, end_index), axis=0)\n",
    "\n",
    "y_validation = np.delete(y_validation, range(start_index, end_index), axis=0)\n",
    "\n",
    "for i in range(len(x_testing)):\n",
    "    start_index = (x_testing[i].shape[0] - (x_testing[i].shape[0] % BATCH_SIZE))\n",
    "    end_index = x_testing[i].shape[0]\n",
    "\n",
    "    x_testing[i] = np.delete(x_testing[i], range(start_index, end_index), axis=0)\n",
    "\n",
    "y_testing = np.delete(y_testing, range(start_index, end_index), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_training is {}, length: {}, member shape: {}.\".format(type(x_training), len(x_training), x_training[0].shape))\n",
    "print(\"y_training is {}, length: {}, member shape: {}.\".format(type(y_training), len(y_training), y_training[0].shape))\n",
    "print()\n",
    "print(\"x_validation is {}, length: {}, member shape: {}.\".format(type(x_validation), len(x_validation), x_validation[0].shape))\n",
    "print(\"y_validation is {}, length: {}, member shape: {}.\".format(type(y_validation), len(y_validation), y_validation[0].shape))\n",
    "print()\n",
    "print(\"x_testing is {}, length: {}, member shape: {}.\".format(type(x_testing), len(x_testing), x_testing[0].shape))\n",
    "print(\"y_testing is {}, length: {}, member shape: {}.\".format(type(y_testing), len(y_testing), y_testing[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build parallel LSTM model\n",
    "The general model architecture is as follows - 410 input layers, one for each geospatial bin. Each input feeds into it's own LSTM layer. The LSTM layers are then concatenated and make there way through a number of fully connected layers and finally to a 410 member sigmoid activated output.\n",
    "\n",
    "The loss function used is weighted binary cross-entropy. As this is a multilabel classification, not a multiclass classification softmax and categorical cross-entropy are not good choices because we can have more than one positive output amount the 410 for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters (adapted from single LSTM Cassandra model)\n",
    "learning_rate = 0.001\n",
    "parallel_lstm_units = 50\n",
    "lstm_units = 410 * 5\n",
    "hidden_units = 410 * 5\n",
    "hidden_l1_lambda = 0.1\n",
    "variational_dropout = 0.24\n",
    "\n",
    "# class_0_weight = (1 / (total_observations - num_fires))*(total_observations)/2.0 \n",
    "# class_1_weight = ((1 / num_fires)*(total_observations)/2.0)#*0.8\n",
    "\n",
    "raw_output_bias = np.log([num_fires/(total_observations - num_fires)])[0]\n",
    "output_bias = tf.keras.initializers.Constant(raw_output_bias)\n",
    "\n",
    "print(f'Inital output bias: {raw_output_bias}')\n",
    "print(f'Class 0 weight: {class_0_weight}')\n",
    "print(f'Class 1 weight: {class_1_weight}')\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.TruePositives(thresholds = 0.1, name = \"true_positives\"),\n",
    "    tf.keras.metrics.FalsePositives(thresholds = 0.1, name = \"false_positives\"),\n",
    "    tf.keras.metrics.TrueNegatives(thresholds = 0.1, name = \"true_negatives\"),\n",
    "    tf.keras.metrics.FalseNegatives(thresholds = 0.1, name = \"false_negatives\"),\n",
    "    tf.keras.metrics.Precision(thresholds = 0.1, name = \"precision\"), # TP / (TP + FP)\n",
    "    tf.keras.metrics.Recall(thresholds = 0.1, name = \"recall\"),        # TP / (TP + FN)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9430ad91cb66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# make list of inputs, one for each geospatial bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     inputs.append(\n\u001b[1;32m      7\u001b[0m         keras.Input(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_training' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "LSTMs = []\n",
    "\n",
    "# make list of inputs, one for each geospatial bin\n",
    "for i in range(len(x_training)):\n",
    "    inputs.append(\n",
    "        keras.Input(\n",
    "            batch_shape=(\n",
    "                BATCH_SIZE,\n",
    "                x_training[0].shape[1],\n",
    "                x_training[0].shape[2]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# make list of LSTM layers, one for each geospatial bin\n",
    "for i in range(len(x_training)):\n",
    "    LSTMs.append(keras.layers.LSTM(\n",
    "        parallel_lstm_units,\n",
    "        dropout = variational_dropout,\n",
    "        stateful = True,\n",
    "        return_sequences = True\n",
    "    )(inputs[i]))\n",
    "    \n",
    "# merge LSTM layers with concat layer\n",
    "merged = keras.layers.concatenate(LSTMs)\n",
    "\n",
    "# sequence of fully connected layers\n",
    "hidden1 = keras.layers.Dense(\n",
    "    hidden_units,\n",
    "    kernel_regularizer=keras.regularizers.l1(hidden_l1_lambda),\n",
    "    bias_initializer=tf.keras.initializers.he_normal(),\n",
    "    activation = 'relu'\n",
    ")(merged)\n",
    "\n",
    "hidden2 = keras.layers.Dense(\n",
    "    (hidden_units // 2),\n",
    "    kernel_regularizer=keras.regularizers.l1(hidden_l1_lambda),\n",
    "    bias_initializer=tf.keras.initializers.he_normal(),\n",
    "    activation = 'relu'\n",
    ")(hidden1)\n",
    "\n",
    "hidden3 = keras.layers.Dense(\n",
    "    (hidden_units // 3),\n",
    "    kernel_regularizer=keras.regularizers.l1(hidden_l1_lambda),\n",
    "    bias_initializer=tf.keras.initializers.he_normal(),\n",
    "    activation = 'relu'\n",
    ")(hidden2)\n",
    "\n",
    "# output layer\n",
    "output = keras.layers.Dense(\n",
    "    410,\n",
    "    activation = 'sigmoid',\n",
    "    bias_initializer = output_bias\n",
    ")(hidden2)\n",
    "\n",
    "# compile model\n",
    "model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr = learning_rate),\n",
    "    loss = weighted_bce(class_0_weight, class_1_weight),\n",
    "    metrics = metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train and evaluate parallel LSTM model\n",
    "The general model architecture is as follows - 410 input layers, one for each geospatial bin. Each input feeds into it's own LSTM layer. The LSTM layers are then concatenated and make there way through a number of fully connected layers and finally to a 410 member sigmoid activated output.\n",
    "\n",
    "The loss function used is weighted binary cross-entropy. As this is a multilabel classification, not a multiclass classification softmax and categorical cross-entropy are not good choices because we can have more than one positive output amount the 410 for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = (x_training[0].shape[0] * 0.99) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (x_validation[0].shape[0] * 0.99) // BATCH_SIZE\n",
    "\n",
    "# Use early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=0,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_training, \n",
    "    y_training,\n",
    "    batch_size = BATCH_SIZE, \n",
    "    epochs = 40,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    validation_steps = VALIDATION_STEPS,\n",
    "    validation_data = (x_validation, y_validation),\n",
    "    workers = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =  ['loss', 'recall', 'true_positives', 'true_negatives', 'false_positives', 'false_negatives']\n",
    "filename = '../../figures/parallel_LSTM_learning_curves_CA_only.png'\n",
    "print(f\"Total observations: {len(training_data)}\")\n",
    "plot_metrics(history, metrics, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = model.predict(x_training)\n",
    "training_ignition_risk_predictions = training_predictions.flatten()\n",
    "training_true_fires = y_training.flatten()\n",
    "\n",
    "testing_predictions = model.predict(x_testing)\n",
    "testing_ignition_risk_predictions = testing_predictions.flatten()\n",
    "testing_true_fires = y_testing.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(training_ignition_risk_predictions, cumulative=False, bw=0.01)\n",
    "plt.xlim(-0.1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = model.evaluate(\n",
    "    x_training,\n",
    "    y_training \n",
    ")\n",
    "\n",
    "testing_results = model.evaluate(\n",
    "    x_testing,\n",
    "    y_testing \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in zip(model.metrics_names, training_results):\n",
    "    print(name, ': ', value)\n",
    "    \n",
    "plot_cm(training_true_fires, training_ignition_risk_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in zip(model.metrics_names, testing_results):\n",
    "    print(name, ': ', value)\n",
    "    \n",
    "plot_cm(testing_true_fires, testing_ignition_risk_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save trained weights for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained weights\n",
    "model.save_weights('./trained_model_weights/parallel_LSTM_weights.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock up some data to match what the live prediction data will look like to ensure the model works as expected after round tripping the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 5\n",
    "future_target = 1\n",
    "step = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "x_training, y_training = format_features_labels_for_LSTM(\n",
    "    training_data,        # incomming_data\n",
    "    past_history,         # size of past history time chunk\n",
    "    future_target,        # number of future timepoints to predict from each history time chunk\n",
    "    step,                 # number of timepoints to move the history time chunk as we slide over the data\n",
    ")\n",
    "\n",
    "x_training, y_training = trim_and_reshape_for_LSTM(x_training, y_training)\n",
    "\n",
    "for i in range(len(x_training)):\n",
    "    start_index = (x_training[i].shape[0] - (x_training[i].shape[0] % BATCH_SIZE))\n",
    "    end_index = x_training[i].shape[0]\n",
    "\n",
    "    x_training[i] = np.delete(x_training[i], range(start_index, end_index), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = []\n",
    "\n",
    "for spatial_bin in x_training:\n",
    "    bin_slice = spatial_bin[0:7,:,]\n",
    "    mock_data.append(bin_slice)\n",
    "    \n",
    "mock_labels = y_training[0:7]\n",
    "    \n",
    "print(\"x_training is {}, length: {}, member shape: {}.\".format(type(x_training), len(x_training), x_training[0].shape))\n",
    "print(\"y_training is {}, length: {}, member shape: {}.\".format(type(y_training), len(y_training), y_training[0].shape))\n",
    "print(\"mock_data is {}, length: {}, member shape: {}.\".format(type(mock_data), len(mock_data), mock_data[0].shape))\n",
    "print(\"mock_labels are {}, length: {}, member shape: {}.\".format(type(mock_labels), len(mock_labels), mock_labels[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "LSTMs = []\n",
    "\n",
    "for i in range(len(mock_data)):\n",
    "    inputs.append(\n",
    "        keras.Input(\n",
    "            batch_shape=(\n",
    "                BATCH_SIZE,\n",
    "                mock_data[0].shape[1],\n",
    "                mock_data[0].shape[2]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "for i in range(len(mock_data)):\n",
    "    LSTMs.append(keras.layers.LSTM(\n",
    "        lstm_units,\n",
    "        dropout = variational_dropout,\n",
    "        stateful = True\n",
    "    )(inputs[i]))\n",
    "    \n",
    "merged = keras.layers.concatenate(LSTMs)\n",
    "\n",
    "hidden1 = keras.layers.Dense(\n",
    "    hidden_units,\n",
    "    kernel_regularizer=keras.regularizers.l1(hidden_l1_lambda),\n",
    "    bias_initializer=tf.keras.initializers.he_normal(),\n",
    "    activation = 'relu'\n",
    ")(merged)\n",
    "\n",
    "hidden2 = keras.layers.Dense(\n",
    "    hidden_units,\n",
    "    kernel_regularizer=keras.regularizers.l1(hidden_l1_lambda),\n",
    "    bias_initializer=tf.keras.initializers.he_normal(),\n",
    "    activation = 'relu'\n",
    ")(hidden1)\n",
    "\n",
    "output = keras.layers.Dense(\n",
    "    410,\n",
    "    activation = 'sigmoid',\n",
    "    bias_initializer = output_bias\n",
    ")(hidden2)\n",
    "\n",
    "production_model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "production_model.load_weights('./trained_model_weights/parallel_LSTM_weights.tf')\n",
    "production_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_predictions = production_model.predict(mock_data)\n",
    "flat_mock_predictions = mock_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(flat_mock_predictions, cumulative=False, bw=0.01)\n",
    "plt.xlim(-0.1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
