{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "As a first pass at developing a machine learning model to predict California wildfires I will evaluate and tune several different gradient boosting algorithms. The procedure will be as follows:\n",
    "\n",
    "1. **Classifier selection:** Find the best classification algorithm for the data based on F1 score compute time and memory requirements\n",
    "2. **Metaparameter optimization:** Optimize data metaparameters such as rolling window width\n",
    "3. **Class imbalance:** Generate a strategy to deal with the unbalanced nature of the classes\n",
    "4. **Hyperparameter optimization:** Tune model hyperparameters\n",
    "5. **Feature importance:** Investigate feature importance and possibly trim/apply dimensionality reduction techniques to the data\n",
    "6. **Model robustness:** How does the model perform after repeated sampling? Are scores stable?\n",
    "\n",
    "There are two anticipated issues which will need to be dealt with first:\n",
    "\n",
    "1. Large dataset size - current working dataset has 7.3 million observations of 25 variables and this is likely to grow as the project progresses\n",
    "2. Highly imbalanced data (~20 times more observations without fire than with)\n",
    "\n",
    "Future goals are to add several more factors from various data sources including: elevation, population density, time since last fire and total fires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from statistics import mean\n",
    "from memory_profiler import memory_usage\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "import helper_functions.data_functions as data_functions\n",
    "import helper_functions.plotting_functions as plotting_functions\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "raw_data_file = 'training_data_pipeline/data/training_data/1992-2015_california.parquet'\n",
    "\n",
    "rand_seed = 42\n",
    "classifier_jobs = 4\n",
    "optimization_jobs = 2\n",
    "max_jobs = classifier_jobs * optimization_jobs\n",
    "\n",
    "sample_size = 10000\n",
    "repetitions = 3\n",
    "train_test_split_ratio = 0.3\n",
    "cv_folds = 3\n",
    "n_doublings = 4\n",
    "scoring_func = make_scorer(f1_score)\n",
    "scoring_func_name = \"F1 score\"\n",
    "grid_search_iterations = 10\n",
    "\n",
    "plot_grid_resolution = 500\n",
    "contourf_levels = 500\n",
    "\n",
    "weather_variables = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'month',\n",
    "    'air.2m',\n",
    "    'apcp',\n",
    "    'rhum_2m',\n",
    "    'dpt_2m',\n",
    "    'pres_sfc',\n",
    "    'uwnd_10m',\n",
    "    'vwnd_10m',\n",
    "    'vis',\n",
    "    'lcdc',\n",
    "    'hcdc',\n",
    "    'mcdc',\n",
    "    'crain',\n",
    "    'veg',\n",
    "    'ignition'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/fastparquet/encoding.py:222: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  Numpy8 = numba.jitclass(spec8)(NumpyIO)\n",
      "/home/siderealyear/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/fastparquet/encoding.py:224: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  Numpy32 = numba.jitclass(spec32)(NumpyIO)\n",
      "/home/siderealyear/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_data_pipeline/data/training_data/1992-2015_california.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/fastparquet/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, verify, open_with, root, sep)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/fastparquet/util.py\u001b[0m in \u001b[0;36mdefault_open\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data_pipeline/data/training_data/1992-2015_california.parquet/_metadata'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-470e76e4a0a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepath_or_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mparquet_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparquet_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/fastparquet/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, verify, open_with, root, sep)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wildfire_production/lib/python3.7/site-packages/fastparquet/util.py\u001b[0m in \u001b[0;36mdefault_open\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data_pipeline/data/training_data/1992-2015_california.parquet'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "raw_data = pd.read_parquet(raw_data_file)\n",
    "raw_data['year'] = raw_data['date_time'].dt.year.astype('float32')\n",
    "raw_data['month'] = raw_data['date_time'].dt.month.astype('float32')\n",
    "raw_data['day'] = raw_data['date_time'].dt.day.astype('float32')\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data[weather_variables]\n",
    "\n",
    "# Shuffel row order\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier selection: kitchen sink approach\n",
    "\n",
    "First step is to throw a bunch of different classifiers with default settings at the problem and see how they do. See list below for contenders.\n",
    "\n",
    "We will start with a small sample of data to quickly get a sense of how well each classifier works. Hopefully we can discard some to make test run times shorter in the next round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn classifiers\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# List of classifier descriptions for plotting / dataframes\n",
    "classifier_descriptions = [\n",
    "    'Logit. regres.',\n",
    "    'Decision tree',\n",
    "    'K neighbors',\n",
    "    'Rand. forest',\n",
    "    'QDA',\n",
    "    'XGBoost',\n",
    "    'CatBoost',\n",
    "    'LightGBM',\n",
    "    'AdaBoost',\n",
    "    'Linear SVM',\n",
    "    'RBF SVM',\n",
    "    'Naive Bayes'\n",
    "]\n",
    "\n",
    "# Classifiers with parameters. Set parallism and rand_seed if avalible.\n",
    "# Also set 'rule of thumb' class weight if avalible for that classifier.\n",
    "\n",
    "num_negative_observations = len(raw_data[raw_data['ignition'] == 0])\n",
    "num_positive_observations = len(raw_data[raw_data['ignition'] == 1])\n",
    "class_ratio = num_negative_observations / num_positive_observations\n",
    "\n",
    "class_weight_dict = {\n",
    "    0: 1,\n",
    "    1: class_ratio\n",
    "}\n",
    "\n",
    "classifiers = (\n",
    "    \n",
    "    LogisticRegression(\n",
    "        n_jobs = max_jobs, \n",
    "        random_state = rand_seed,\n",
    "        class_weight = class_weight_dict\n",
    "    ),\n",
    "    \n",
    "    DecisionTreeClassifier( \n",
    "        random_state = rand_seed,\n",
    "        class_weight = class_weight_dict\n",
    "    ),\n",
    "    \n",
    "    KNeighborsClassifier(\n",
    "        n_jobs = max_jobs        \n",
    "    ),\n",
    "    \n",
    "    RandomForestClassifier(\n",
    "        n_jobs = max_jobs,\n",
    "        class_weight = class_weight_dict,\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    \n",
    "    XGBClassifier(\n",
    "        n_jobs = max_jobs,\n",
    "        scale_pos_weight = class_ratio,\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    CatBoostClassifier(\n",
    "        thread_count = max_jobs, \n",
    "        silent = True,\n",
    "        scale_pos_weight = class_ratio,\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    LGBMClassifier(\n",
    "        n_jobs = max_jobs,\n",
    "        scale_pos_weight = class_ratio,\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    AdaBoostClassifier(random_state = rand_seed),\n",
    "    \n",
    "    SVC(kernel = \"linear\",\n",
    "        class_weight = 'balanced',\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    SVC(\n",
    "        class_weight = 'balanced',\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    GaussianNB(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over classifier list, run k-fold cross validation on stratified \n",
    "# training sample for each. Report mean and SD of score, run time and peak memory use\n",
    "\n",
    "# Start empty dataframe to hold results\n",
    "classifier_data = pd.DataFrame(columns=[\n",
    "    'Classifier',\n",
    "    'Sample size',\n",
    "    'CV folds',\n",
    "    'Raw scores',\n",
    "    'Mean {} score'.format(scoring_func_name),\n",
    "    '{} score SD'.format(scoring_func_name),\n",
    "    'Peak memory (GB)',\n",
    "    'Run time (min.)'\n",
    "])\n",
    "\n",
    "# Loop over classifiers\n",
    "for classifier, description in zip(classifiers, classifier_descriptions):\n",
    "    \n",
    "    dTs = []\n",
    "    max_mems = []\n",
    "    raw_scores = []\n",
    "    \n",
    "    for i in range(repetitions):\n",
    "        \n",
    "        # Take stratified sample data of data and do stratified train_test split    \n",
    "        X_train, X_test, y_train, y_test = data_functions.make_train_test_sample(\n",
    "            data, \n",
    "            sample_size, \n",
    "            train_test_split_ratio, \n",
    "            rand_seed\n",
    "        )\n",
    "        \n",
    "        start = time()\n",
    "\n",
    "        # Cross validate classifier, tracking memory usage\n",
    "        mem_usage, cross_val_scores = memory_usage((data_functions.cross_validate_classifier, (\n",
    "            classifier, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            cv_folds, \n",
    "            scoring_func)\n",
    "        ), retval=True)\n",
    "\n",
    "        dT = np.round(((time() - start)/60), 2)\n",
    "        dTs.append(dT)\n",
    "        raw_scores = np.concatenate((raw_scores, cross_val_scores))\n",
    "        max_mems = np.concatenate((max_mems, mem_usage))\n",
    "\n",
    "    mean_score = np.round(mean(raw_scores), 3)\n",
    "    sd_score = np.round(np.std(raw_scores), 4)\n",
    "    max_mem = np.round((max(max_mems) / 1000), 3)\n",
    "    \n",
    "    # Store run results in dataframe\n",
    "    classifier_data = classifier_data.append(pd.Series([\n",
    "        description,\n",
    "        sample_size,\n",
    "        cv_folds,\n",
    "        cross_val_scores,\n",
    "        mean_score,\n",
    "        sd_score,\n",
    "        max_mem,\n",
    "        dT\n",
    "    ], index=classifier_data.columns), ignore_index=True)\n",
    "    \n",
    "    print(f'Finished {description}, mean score: {mean_score}')\n",
    "\n",
    "# Sort results by score\n",
    "classifier_data = classifier_data.sort_values(['Mean {} score'.format(scoring_func_name)], ascending=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of classifiers using mean score\n",
    "fig = plt.figure(1, figsize=(12, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "bp = ax.boxplot(classifier_data['Raw scores'])\n",
    "ax.set_xticklabels(classifier_data['Classifier'])\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('{} score'.format(scoring_func_name))\n",
    "plt.title('Classifier comparision, 5-fold cross validation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_data.drop('Raw scores', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, our top three winners are gradient boosting algorithms. All three are within a standard deviation of each other, but CatBoost takes ~15 times longer than the other two. Let's focus on the top three and quickly check how well they will scale as the dataset grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier selection: resource requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced classifier set\n",
    "\n",
    "classifier_descriptions = [\n",
    "    'LightGBM',\n",
    "    'CatBoost',\n",
    "    'XGBoost'\n",
    "]\n",
    "\n",
    "classifiers = (\n",
    "    \n",
    "    LGBMClassifier(\n",
    "        n_jobs = max_jobs,\n",
    "        scale_pos_weight = class_ratio,\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    CatBoostClassifier(\n",
    "        thread_count = max_jobs, \n",
    "        silent = True,\n",
    "        scale_pos_weight = class_ratio,\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    "    \n",
    "    XGBClassifier(\n",
    "        n_jobs = max_jobs,\n",
    "        scale_pos_weight = class_ratio,\n",
    "        random_state = rand_seed\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_data = pd.DataFrame(columns=[\n",
    "    'Classifier',\n",
    "    'Sample size',\n",
    "    'Peak memory (GB)',\n",
    "    'Run time (min.)'\n",
    "])\n",
    "\n",
    "size_of_sample = 5000\n",
    "\n",
    "# Loop over range n, doubling the size of the dataset sample each time\n",
    "for i in range(0,(n_doublings+1)):\n",
    "    \n",
    "    # Loop over each classifier, store max memory and run time\n",
    "    for classifier, description in zip(classifiers, classifier_descriptions):\n",
    "    \n",
    "        for i in range(repetitions):\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = data_functions.make_train_test_sample(\n",
    "                data, \n",
    "                size_of_sample, \n",
    "                train_test_split_ratio, \n",
    "                rand_seed\n",
    "            )\n",
    "            \n",
    "            start = time()\n",
    "            mem_usage, model = memory_usage((data_functions.fit_model, (classifier, X_train, y_train)), retval=True)\n",
    "            dT = np.round(((time() - start)/60),2)\n",
    "\n",
    "            max_mem = np.round((max(mem_usage) / 1000),3)\n",
    "\n",
    "            sample_size_data = sample_size_data.append(pd.Series([\n",
    "                description,\n",
    "                size_of_sample,\n",
    "                max_mem,\n",
    "                dT\n",
    "            ], index=sample_size_data.columns), ignore_index=True)\n",
    "    \n",
    "    size_of_sample = size_of_sample * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='Sample size', y='Run time (min.)', hue='Classifier', data=sample_size_data)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Run time vs. data sample size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='Sample size', y='Peak memory (GB)', hue='Classifier', data=sample_size_data)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Peak memory vs. data sample size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like XGBoost is the holistic winner. XGBoost's cross validation scores are close to those of CatBoost and it seems to be significantly faster. It does appear to use slightly more memory, hopefully that won't be a problem. Let's look at it in a bit more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain default model with standard data sample size\n",
    "model = XGBClassifier(\n",
    "    n_jobs = max_jobs,\n",
    "    scale_pos_weight = class_ratio,\n",
    "    random_state = rand_seed\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_functions.make_train_test_sample(\n",
    "    data, \n",
    "    sample_size, \n",
    "    train_test_split_ratio, \n",
    "    rand_seed\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = np.array(['No ignition', 'Ignition'])\n",
    "plotting_functions.display_confusion_matrix(model, class_names, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate false positive and negative rates\n",
    "false_neg_rate, false_pos_rate = data_functions.calc_false_neg_pos_rate(model, X_test, y_test)\n",
    "print(\"False negative rate: {}\".format(np.round(false_neg_rate, 2)))\n",
    "print(\"False positive rate: {}\".format(np.round(false_pos_rate, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the nature of the problem, false negative are more dangerous than false positives. Although our false negative rate is under 10%, even with this small sample, we missed a significant number of fires. Needless to say, there is room for improvement here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metaparameter optimization: to smooth or not to smooth\n",
    "\n",
    "You may have notices that I loaded three different datasets at the start of this notebook:\n",
    "\n",
    "1. **raw_data:** No significant manipulations beyond collating the various datasets. Has a time resolution of 3 hr. due to the frequency of the original NOAA weather data set.\n",
    "2. **data_rolling_window:** Data was reversed with regard to time and then averaged with a 6 hr. rolling window. This has the effect of assigning the window mean to the time bin at the left of the window. This was done because logically we should care more about weather conditions before the fire was discovered than after.\n",
    "3. **data_daily_mean:** Rolling window data was averaged by day.\n",
    "\n",
    "**Note:** All averaging includes the boolean ignition value. After averaging is complete any noxel with a non-zero ignition value is reassigned a 1. This tends to make the fires discovery 'spread' to earlier times. Again, this is acceptable and even desirable because logically the fire must have started before it was discovered.\n",
    "\n",
    "For more details about these three datasets take a look at the [data preprocessing notebook](https://github.com/gperdrizet/wildfire/blob/master/notebooks/preprocess_data.ipynb).\n",
    "\n",
    "OK, so, let's compare the three sets with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll data with windowed mean\n",
    "data_rolling_window = raw_data.iloc[::-1]\n",
    "data_rolling_window = data_rolling_window.groupby(['lat', 'lon']).rolling(48, on=\"date_time\").mean()\n",
    "data_rolling_window.reset_index(drop=True, inplace=True)\n",
    "data_rolling_window.drop('date_time', axis=1, inplace=True)\n",
    "\n",
    "# Reassign fractional ignition values a '1'\n",
    "ignitions = data_rolling_window[data_rolling_window['ignition'] > 0].copy()\n",
    "no_ignitions = data_rolling_window[data_rolling_window['ignition'] == 0].copy()\n",
    "ignitions['ignition'] = 1\n",
    "\n",
    "data_rolling_window = ignitions.append(no_ignitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average data by day\n",
    "data_daily_mean = data_rolling_window.groupby([\n",
    "    'lat', \n",
    "    'lon', \n",
    "    'year',\n",
    "    'month',\n",
    "    'day'\n",
    "]).mean().reset_index()\n",
    "\n",
    "data_daily_mean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Reassign fractional ignition values a '1'\n",
    "ignitions = data_daily_mean[data_daily_mean['ignition'] > 0].copy()\n",
    "no_ignitions = data_daily_mean[data_daily_mean['ignition'] == 0].copy()\n",
    "ignitions['ignition'] = 1\n",
    "\n",
    "data_daily_mean = ignitions.append(no_ignitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rolling_window.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_daily_mean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over dataset list, run k-fold cross validation on stratified training sample\n",
    "# from each. Report raw scores.\n",
    "\n",
    "dataset_descriptions = [\n",
    "    \"Raw data\",\n",
    "    \"Rolling window\",\n",
    "    \"Daily average\"\n",
    "]\n",
    "\n",
    "datasets = (\n",
    "    data,\n",
    "    data_rolling_window,\n",
    "    data_daily_mean\n",
    ")\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "    n_jobs = max_jobs,\n",
    "    scale_pos_weight = class_ratio,\n",
    "    random_state = rand_seed\n",
    ")\n",
    "\n",
    "# Start empty dataframe to hold results\n",
    "dataset_comparison_results = pd.DataFrame(columns=[\n",
    "    'Data type',\n",
    "    'Raw scores',\n",
    "])\n",
    "\n",
    "# Loop on datasets\n",
    "for dataset, description in zip(datasets, dataset_descriptions):\n",
    "    \n",
    "    cross_val_scores = []\n",
    "    \n",
    "    for i in range(repetitions):\n",
    "  \n",
    "        # Draw train-test sample from dataset\n",
    "        X_train, X_test, y_train, y_test = data_functions.make_train_test_sample(\n",
    "            dataset, \n",
    "            sample_size, \n",
    "            train_test_split_ratio, \n",
    "            rand_seed\n",
    "        )\n",
    "\n",
    "        # Crossvalidate on training data\n",
    "        scores = data_functions.cross_validate_classifier(\n",
    "            classifier, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            cv_folds, \n",
    "            scoring_func\n",
    "        )\n",
    "        \n",
    "        cross_val_scores.append(scores)\n",
    "\n",
    "    # Store raw scores\n",
    "    dataset_comparison_results = dataset_comparison_results.append(pd.Series([\n",
    "        description,\n",
    "        cross_val_scores,\n",
    "    ], index=dataset_comparison_results.columns), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparision of datasets by score\n",
    "fig = plt.figure(1, figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "bp = ax.boxplot(dataset_comparison_results['Raw scores'])\n",
    "ax.set_xticklabels(dataset_comparison_results['Data type'])\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('CV {} score'.format(scoring_func_name))\n",
    "plt.title('Dataset comparision')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like the daily averages are the winner, though the rolling window is close. Before moving on, let's see what happens if we try to optimize the rolling window width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metaparameter optimization: rolling window width\n",
    "\n",
    "The use of a rolling window introduces a possibly extremely important metaparameter: the rolling window width. Let's play with that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over rolling window sizes, run k-fold cross validation on stratified \n",
    "# training sample for each. Report raw scores, false positive and false negative rates.\n",
    "\n",
    "window_widths = range(2, 10, 1)\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "    n_jobs = max_jobs,\n",
    "    scale_pos_weight = class_ratio,\n",
    "    random_state = rand_seed\n",
    ")\n",
    "\n",
    "# Start empty dataframe to hold results\n",
    "model_scores = pd.DataFrame(columns=[\n",
    "    'Window width',\n",
    "    'CV {} score'.format(scoring_func_name),\n",
    "    'CV False positive rate',\n",
    "    'CV False negative rate'\n",
    "])\n",
    "\n",
    "# Loop over window width\n",
    "for window_width in window_widths:\n",
    "    \n",
    "    # Roll dataset with window \n",
    "    data_rolling_window = raw_data.iloc[::-1]\n",
    "    data_rolling_window = data_rolling_window.groupby(['lat', 'lon']).rolling(window_width, on=\"date_time\").mean()\n",
    "    data_rolling_window.reset_index(drop=True, inplace=True)\n",
    "    data_rolling_window.drop('date_time', axis=1, inplace=True)\n",
    "    \n",
    "    ignitions = data_rolling_window[data_rolling_window['ignition'] > 0].copy()\n",
    "    no_ignitions = data_rolling_window[data_rolling_window['ignition'] == 0].copy()\n",
    "    ignitions['ignition'] = 1\n",
    "\n",
    "    dataset = ignitions.append(no_ignitions)\n",
    "    \n",
    "    # Draw train-test sample from dataset\n",
    "    X_train, X_test, y_train, y_test = data_functions.make_train_test_sample(\n",
    "        dataset, \n",
    "        sample_size, \n",
    "        train_test_split_ratio, \n",
    "        rand_seed\n",
    "    )\n",
    "    \n",
    "    # Run cross validation on traing data\n",
    "    cross_val_scores = data_functions.cross_validate_classifier(\n",
    "        classifier, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        cv_folds, \n",
    "        scoring_func\n",
    "    )\n",
    "\n",
    "    score = mean(cross_val_scores)\n",
    "    \n",
    "    # Run cross validation with false positive rate\n",
    "    cross_val_FP_rates = data_functions.cross_validate_classifier(\n",
    "        classifier, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        cv_folds, \n",
    "        make_scorer(data_functions.FP_rate_scorer)\n",
    "    ) \n",
    "\n",
    "    cross_val_FP_rate = mean(cross_val_FP_rates)\n",
    "\n",
    "    # Run cross validation with false negative rate\n",
    "    cross_val_FN_rates = data_functions.cross_validate_classifier(\n",
    "        classifier, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        cv_folds, \n",
    "        make_scorer(data_functions.FN_rate_scorer)\n",
    "    ) \n",
    "\n",
    "    cross_val_FN_rate = mean(cross_val_FN_rates)\n",
    "\n",
    "    # Store results\n",
    "    model_scores = model_scores.append(pd.Series([\n",
    "        window_width,\n",
    "        np.round(score,3), \n",
    "        np.round(cross_val_FP_rate,3), \n",
    "        np.round(cross_val_FN_rate,3)\n",
    "    ], index=model_scores.columns), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot false positive and negative rates as a function of window size\n",
    "plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    model_scores['Window width'], \n",
    "    model_scores['CV False negative rate'],\n",
    "    color = \"darkred\",\n",
    "    label = 'False negatives'\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    model_scores['Window width'], \n",
    "    model_scores['CV False positive rate'],\n",
    "    color = \"darkblue\",\n",
    "    label = 'False positives'\n",
    ")\n",
    "\n",
    "plt.xlabel('Window width (hr.)')\n",
    "plt.ylabel('Average CV rate')\n",
    "plt.title('Window width vs. false classification')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    model_scores['Window width'], \n",
    "    model_scores['CV {} score'.format(scoring_func_name)],\n",
    "    color = \"darkgreen\",\n",
    "    label = 'False positives'\n",
    ")\n",
    "plt.xlabel('Window width (hr.)')\n",
    "plt.ylabel('Average CV rate')\n",
    "plt.title('Window width vs. CV {} score'.format(scoring_func_name))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization: class weight\n",
    "\n",
    "One of the challenges inherent in this dataset is imbalanced classes. An appripriate class weight can help aleviate this. Also, one of the strange consequences of increasing the window width is an increase in the number of observations labeled fire. Now that we have the window width metaparameter nailed down, let's optimize the class weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class weight search space\n",
    "class_weights = np.logspace(-2, 3, num=5, base=10)\n",
    "\n",
    "# Run cross validation on each class weight, report score\n",
    "# false positive and false negative rates\n",
    "model_scores = data_functions.tune_class_weight(\n",
    "    class_weights,\n",
    "    max_jobs,\n",
    "    rand_seed,\n",
    "    data_daily_mean,\n",
    "    repetitions,\n",
    "    sample_size,\n",
    "    train_test_split_ratio,\n",
    "    cv_folds,\n",
    "    scoring_func_name,\n",
    "    scoring_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_func_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot false positive rate, false negative rate and score\n",
    "# as a function of class weight\n",
    "plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    np.log10(model_scores['Class weight']), \n",
    "    model_scores['CV False positive rate'],\n",
    "    color = \"darkblue\",\n",
    "    label = 'False positives'\n",
    ")\n",
    "plt.scatter(\n",
    "    np.log10(model_scores['Class weight']), \n",
    "    model_scores['CV False negative rate'],\n",
    "    color = \"darkred\",\n",
    "    label = 'False negatives'\n",
    ")\n",
    "plt.xlabel('Log10 class weight ratio')\n",
    "plt.ylabel('Cross validation rate')\n",
    "plt.title('Class weight vs. false positives & negatives')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    np.log10(model_scores['Class weight']), \n",
    "    model_scores['CV {} score'.format(scoring_func_name)],\n",
    "    color = \"darkgreen\",\n",
    "    label ='CV {} score'.format(scoring_func_name),\n",
    ")\n",
    "plt.xlabel('Log10 class weight ratio')\n",
    "plt.ylabel('CV {} score'.format(scoring_func_name))\n",
    "plt.title('Class weight vs. {} score'.format(scoring_func_name))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Interesting. We can use larger class weights to drive down the false negative rate at the expense of false positives. Looking at the clearly sigmoidal score curve, the optimum value seems to be between 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick and store the class wight which gives the highest score\n",
    "model_scores = model_scores.sort_values(['CV {} score'.format(scoring_func_name)], ascending=[0])\n",
    "model_scores.reset_index(drop=True, inplace=True)\n",
    "class_weight = model_scores.loc[0,'Class weight']\n",
    "print(\"{} optimized class weight: {}\".format(scoring_func_name,np.round(class_weight,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Important to keep in mind here that we can tune our false positive/false negative rates easily with this hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning: learning rate and tree count\n",
    "\n",
    "First, let's try taking one sample of the full dataset and then using RandomizedSearchCV to try and find the best values for learning rate and tree count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier whith all the hyperparameters we have determined so far\n",
    "classifier = XGBClassifier(\n",
    "    n_jobs = classifier_jobs,\n",
    "    scale_pos_weight = class_weight,\n",
    "    random_state = rand_seed\n",
    ")\n",
    "\n",
    "# Set search space\n",
    "param_dist = {\n",
    "    'learning_rate': loguniform(0.0001, 1),\n",
    "    'n_estimators': range(1,200),\n",
    "    'max_depth': range(1, 21, 1),\n",
    "    'gamma': np.linspace(0, 10, 101),\n",
    "    'min_child_weight': loguniform(0.0001, 10),\n",
    "    'reg_lambda': loguniform(0.0001, 10)\n",
    "    \n",
    "}\n",
    "\n",
    "# Run randomsearchCV\n",
    "best_model, random_search = tune_hyperparameters(\n",
    "    classifier,\n",
    "    param_dist, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    optimization_jobs, \n",
    "    grid_search_iterations, \n",
    "    scoring_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results to dataframe\n",
    "rand_search_results = pd.DataFrame(random_search.cv_results_).dropna()\n",
    "\n",
    "# Make desnity plot showing the dependenc of score on\n",
    "# learning rate and n_estimators\n",
    "x = rand_search_results['param_n_estimators']\n",
    "y = rand_search_results['param_learning_rate']\n",
    "z = rand_search_results['mean_test_score']\n",
    "xi, yi, zi = regularize_grid(x, y, z, plot_grid_resolution)\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "plt.contourf(xi, yi, zi, contourf_levels, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"N estimators\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.title(\"Effect of estimator count and \\nlearning rate on score\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprised by how 'rough' hyperparameter space is. Let's keep the winning numbers and save the scores to our log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = rand_search_results.iloc[0]['param_learning_rate']\n",
    "best_n_estimators = rand_search_results.iloc[0]['param_n_estimators']\n",
    "print(\"{} optimized learning rate: {}\".format(scoring_func_name,np.round(best_learning_rate,2)))\n",
    "print(\"{} optimized n estimators: {}\".format(scoring_func_name,np.round(best_n_estimators,2)))\n",
    "\n",
    "best_depth = rand_search_results.iloc[0]['param_max_depth']\n",
    "best_gamma = rand_search_results.iloc[0]['param_gamma']\n",
    "print(\"{} optimized tree depth: {}\".format(scoring_func_name,np.round(best_depth,2)))\n",
    "print(\"{} optimized gamma: {}\".format(scoring_func_name,np.round(best_gamma,2)))\n",
    "\n",
    "best_min_child_weight = rand_search_results.iloc[0]['param_min_child_weight']\n",
    "best_reg_lambda = rand_search_results.iloc[0]['param_reg_lambda']\n",
    "print(\"{} optimized min. child weight: {}\".format(scoring_func_name,np.round(best_min_child_weight,2)))\n",
    "print(\"{} optimized L2 regularization: {}\".format(scoring_func_name,np.round(best_reg_lambda,2)))\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = np.array(['No ignition', 'Ignition'])\n",
    "display_confusion_matrix(best_model, class_names, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning: tree depth and gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier whith all the hyperparameters we have determined so far\n",
    "classifier = XGBClassifier(\n",
    "    n_jobs = classifier_jobs,\n",
    "    scale_pos_weight = class_weight,\n",
    "    random_state = rand_seed,\n",
    "    learning_rate = best_learning_rate,\n",
    "    n_estimators = best_n_estimators\n",
    ")\n",
    "\n",
    "# Set search space\n",
    "param_dist = {\n",
    "    'max_depth': range(1, 21, 1),\n",
    "    'gamma': np.linspace(0, 10, 101)\n",
    "}\n",
    "\n",
    "# Run randomsearchCV\n",
    "best_model, random_search = tune_hyperparameters(\n",
    "    classifier,\n",
    "    param_dist, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    optimization_jobs, \n",
    "    grid_search_iterations, \n",
    "    scoring_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results to dataframe\n",
    "rand_search_results = pd.DataFrame(random_search.cv_results_).dropna()\n",
    "\n",
    "# Make desnity plot showing the dependenc of score on\n",
    "# learning rate and n_estimators\n",
    "x = rand_search_results['param_max_depth']\n",
    "y = rand_search_results['param_gamma']\n",
    "z = rand_search_results['mean_test_score']\n",
    "xi, yi, zi = regularize_grid(x, y, z, plot_grid_resolution)\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "plt.contourf(xi, yi, zi, contourf_levels, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.ylabel(\"Gamma\")\n",
    "plt.title(\"Effect of tree depth and gamma on score\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again store winning parameter for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = rand_search_results.iloc[0]['param_max_depth']\n",
    "best_gamma = rand_search_results.iloc[0]['param_gamma']\n",
    "print(\"F1 optimized tree depth: {}\".format(np.round(best_depth,2)))\n",
    "print(\"F1 optimized gamma: {}\".format(np.round(best_gamma,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "Now that we have our hyperparameters nailed down, let't take a look at feature importance. First thing to do is train our optimized model on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with tuned hyperparameters\n",
    "classifier = XGBClassifier(\n",
    "    n_jobs = max_jobs,\n",
    "    scale_pos_weight = class_weight,\n",
    "    learning_rate = best_learning_rate,\n",
    "    n_estimators = best_n_estimators,\n",
    "    random_state = rand_seed,\n",
    "    max_depth = best_depth,\n",
    "    gamma = best_gamma\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = make_train_test_sample(\n",
    "    data_daily_mean, \n",
    "    sample_size, \n",
    "    train_test_split_ratio, \n",
    "    rand_seed\n",
    ")\n",
    "\n",
    "# Train model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = np.array(['No ignition', 'Ignition'])\n",
    "display_confusion_matrix(classifier, class_names, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_rate, false_pos_rate = calc_false_neg_pos_rate(classifier, X_test, y_test)\n",
    "print(\"False negative rate: {}\".format(np.round(false_neg_rate, 2)))\n",
    "print(\"False positive rate: {}\".format(np.round(false_pos_rate, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_feature_importance(classifier, data_daily_mean, X_test, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like only our top 5 or so features are actually contributing anything. Let's retrain using only those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab top n feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "importances = classifier.feature_importances_.tolist()\n",
    "indices = np.argsort(importances)[::-1].tolist()\n",
    "bottem_n_features = [feature_names[i] for i in indices[8:-1]]\n",
    "\n",
    "data_daily_mean.drop(bottem_n_features, axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = make_train_test_sample(\n",
    "    data_daily_mean, \n",
    "    sample_size, \n",
    "    train_test_split_ratio, \n",
    "    rand_seed\n",
    ")\n",
    "\n",
    "# rebuild training and test sets with feature subset\n",
    "# X_train, X_test, y_train, y_test = make_train_test_sample(\n",
    "#     data_daily_mean, \n",
    "#     len(data_daily_mean), \n",
    "#     train_test_split_ratio, \n",
    "#     rand_seed\n",
    "# )\n",
    "\n",
    "#X_train, X_test = scale_weather_variables(weather_variables, X_train, X_test)\n",
    "\n",
    "# X_train_subset = X_train[top_n_features]\n",
    "# X_test_subset = X_test[top_n_features]\n",
    "\n",
    "# Train model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = np.array(['No ignition', 'Ignition'])\n",
    "display_confusion_matrix(classifier, class_names, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_rate, false_pos_rate = calc_false_neg_pos_rate(classifier, X_test, y_test)\n",
    "print(\"False negative rate: {}\".format(np.round(false_neg_rate, 2)))\n",
    "print(\"False positive rate: {}\".format(np.round(false_pos_rate, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model: robustness\n",
    "Let's see how our final model holds up to repeated sampling - are the scores variable? or are the consistent across samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = list()\n",
    "test_scores = list()\n",
    "false_neg_rates = list()\n",
    "false_pos_rates = list()\n",
    "\n",
    "sample_size = 5000\n",
    "\n",
    "for i in range(0, 10):\n",
    "    # rebuild training and test sets with feature subset\n",
    "    X_train, X_test, y_train, y_test = make_train_test_sample(\n",
    "        data_daily_mean, \n",
    "        sample_size, \n",
    "        train_test_split_ratio, \n",
    "        rand_seed\n",
    "    )\n",
    "    \n",
    "    #X_train, X_test = scale_weather_variables(weather_variables, X_train, X_test)\n",
    "    \n",
    "    #X_train_subset = X_train[top_n_features]\n",
    "    #X_test_subset = X_test[top_n_features]\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    train_score = f1_score(y_train, classifier.predict(X_train))\n",
    "    test_score = f1_score(y_test, classifier.predict(X_test))\n",
    "        \n",
    "    false_neg_rate, false_pos_rate = calc_false_neg_pos_rate(classifier, X_test, y_test)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    false_neg_rates.append(false_neg_rate)\n",
    "    false_pos_rates.append(false_pos_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "plt.subplots(1, 3, figsize=(25, 7))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "\n",
    "ax = sns.kdeplot(train_scores, label=\"Training data\", shade=True, color=\"darkblue\")\n",
    "ax = sns.kdeplot(test_scores, label=\"Test data\", shade=True, color=\"darkred\")\n",
    "ax.set_title(\"{} score distributions\\nrepeated samples\".format(scoring_func_name))\n",
    "ax.set(xlabel='{} score'.format(scoring_func_name), ylabel='Density')\n",
    "plt.legend()\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "\n",
    "ax = sns.kdeplot(false_neg_rates, label=\"False negative\", shade=True, color=\"darkblue\")\n",
    "ax = sns.kdeplot(false_pos_rates, label=\"False positive\", shade=True, color=\"darkred\")\n",
    "ax.set_title(\"False positive and negative rates\\nrepeated samples\")\n",
    "ax.set(xlabel='Rate', ylabel='Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "\n",
    "plt.scatter(false_neg_rates, false_pos_rates, s=50, color=\"darkblue\")\n",
    "plt.xlabel(\"False negative rate\")\n",
    "plt.ylabel(\"False positive rate\")\n",
    "plt.title(\"False positive vs false negative rate\\nrepeated samples\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall, XGBoost was selected as the best classifier for this problem. The initial model performance was improved by optimizing dataset smoothing and hyperparameters. Then feature importance was investigated and lowly important features pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "The current model can likely be further improved in four ways:\n",
    "1. Apply neural net based machine learning.\n",
    "2. Add more features to the data such as: elevation, total fires at a location etc.\n",
    "3. Reframe the problem as regression by calculating fire probability, rather than using a binary 'yes'-'no' approach.\n",
    "4. Implement 'fully stratified sampling' i.e. choose samples for training, validation and testing such that the distribution of all factors in the sample match the parent dataset as closely as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
