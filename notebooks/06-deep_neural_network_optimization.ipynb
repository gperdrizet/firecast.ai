{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural net optimization\n",
    "Hopefully, the performance of decision tree based models can be improved upon using a deep neural network. The plan is to start small and train on one geospatial bin only. If the model shows promise, it will then be scaled to make predictions for all of California.\n",
    "\n",
    "### Goal: \n",
    "Optimize and test the performance of a deep neural network on one California geospatial bin.\n",
    "\n",
    "### Plan:\n",
    "1. Prep data from one geospatial bin for input into neural network\n",
    "2. Generate 'Cassandra' model -> attempt to over fit and 'memorize' the training data. This will prove that the data is feature rich enough and a the network is complex enough to learn a function which successfully maps input to output.\n",
    "3. If step 2 is successful, regularize model to increase generalizability.\n",
    "4. If step 3 is successful, scale model to predict wilfire risk for all 410 California geospatial bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: tf 2.1.0 give warning about model weight format when\n",
    "# using class weights. This is the only way to silence without\n",
    "# updating\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Note: sklearn forces depreciation warnings\n",
    "# This is the only way to silence them without updating\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_objective_2D #, plot_histogram\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions.data_functions as data_functions\n",
    "import helper_functions.plotting_functions as plotting_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.7 (default, Mar 23 2020, 22:36:06) \n",
      "[GCC 7.3.0]\n",
      "\n",
      "Pandas 1.0.3\n",
      "Tensorflow 2.1.0\n",
      "Keras 2.2.4-tf\n",
      "SciKit Learn 0.22.1\n",
      "\n",
      "tf accessable GPU found: device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python {sys.version}\")\n",
    "print()\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Tensorflow {tf.__version__}\")\n",
    "print(f\"Keras {keras.__version__}\")\n",
    "print(f\"SciKit Learn {sklearn.__version__}\")\n",
    "print()\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "\n",
    "if 'GPU' in ('').join(str(devices)):\n",
    "    print(\"tf accessable GPU found: \"+devices[-2].physical_device_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../training_data_pipeline/data/stratified_training_data/1992-2015_california_features_added_n500000_ks_pval0.3.1.parquet'\n",
    "validation_file = '../training_data_pipeline/data/stratified_training_data/1992-2015_california_features_added_n500000_ks_pval0.3.2.parquet'\n",
    "test_file = '../training_data_pipeline/data/stratified_training_data/1992-2015_california_features_added_n500000_ks_pval0.3.3.parquet'\n",
    "\n",
    "# # Datatypes for dataframe loading\n",
    "# dtypes = {\n",
    "#     'lat': float,\n",
    "#     'lon': float,\n",
    "#     'weather_bin_year': int,\n",
    "#     'weather_bin_month': int,\n",
    "#     'weather_bin_day': int,\n",
    "#     'air.2m': float,\n",
    "#     'apcp': float,\n",
    "#     'rhum.2m': float,\n",
    "#     'dpt.2m': float,\n",
    "#     'pres.sfc': float,\n",
    "#     'uwnd.10m': float,\n",
    "#     'vwnd.10m': float,\n",
    "#     'veg': float,\n",
    "#     'vis': float,\n",
    "#     'ignition': float,\n",
    "#     'mean.air.2m': float,\n",
    "#     'mean.apcp': float,\n",
    "#     'mean.rhum.2m': float,\n",
    "#     'mean.dpt.2m': float,\n",
    "#     'mean.pres.sfc': float,\n",
    "#     'mean.uwnd.10m': float,\n",
    "#     'mean.vwnd.10m': float,\n",
    "#     'mean.veg': float,\n",
    "#     'mean.vis': float,\n",
    "#     'max.air.2m': float,\n",
    "#     'max.apcp': float,\n",
    "#     'max.rhum.2m': float,\n",
    "#     'max.dpt.2m': float,\n",
    "#     'max.pres.sfc': float,\n",
    "#     'max.uwnd.10m': float,\n",
    "#     'max.vwnd.10m': float,\n",
    "#     'max.veg': float,\n",
    "#     'max.vis': float,\n",
    "#     'min.air.2m': float,\n",
    "#     'min.apcp': float,\n",
    "#     'min.rhum.2m': float,\n",
    "#     'min.dpt.2m': float,\n",
    "#     'min.pres.sfc': float,\n",
    "#     'min.uwnd.10m': float,\n",
    "#     'min.vwnd.10m': float,\n",
    "#     'min.veg': float,\n",
    "#     'min.vis': float,\n",
    "#     'total_fires': float\n",
    "\n",
    "# }\n",
    "\n",
    "# # Features to use during training \n",
    "# features = [\n",
    "#     'lat',\n",
    "#     'lon',\n",
    "#     'weather_bin_month',\n",
    "#     'veg',\n",
    "#     'ignition',\n",
    "#     'mean.air.2m',\n",
    "#     'mean.apcp',\n",
    "#     'mean.rhum.2m',\n",
    "#     'mean.dpt.2m',\n",
    "#     'mean.pres.sfc',\n",
    "#     'mean.uwnd.10m',\n",
    "#     'mean.vwnd.10m',\n",
    "#     'mean.vis',\n",
    "#     'max.air.2m',\n",
    "#     'max.apcp',\n",
    "#     'max.rhum.2m',\n",
    "#     'max.dpt.2m',\n",
    "#     'max.pres.sfc',\n",
    "#     'max.uwnd.10m',\n",
    "#     'max.vwnd.10m',\n",
    "#     'max.vis',\n",
    "#     'min.air.2m',\n",
    "#     'min.apcp',\n",
    "#     'min.rhum.2m',\n",
    "#     'min.dpt.2m',\n",
    "#     'min.pres.sfc',\n",
    "#     'min.uwnd.10m',\n",
    "#     'min.vwnd.10m',\n",
    "#     'min.vis',\n",
    "#     'total_fires'\n",
    "# ]\n",
    "\n",
    "# features_to_scale = [\n",
    "#     'lat',\n",
    "#     'lon',\n",
    "#     'veg',\n",
    "#     'mean.air.2m',\n",
    "#     'mean.apcp',\n",
    "#     'mean.rhum.2m',\n",
    "#     'mean.dpt.2m',\n",
    "#     'mean.pres.sfc',\n",
    "#     'mean.uwnd.10m',\n",
    "#     'mean.vwnd.10m',\n",
    "#     'mean.vis',\n",
    "#     'max.air.2m',\n",
    "#     'max.apcp',\n",
    "#     'max.rhum.2m',\n",
    "#     'max.dpt.2m',\n",
    "#     'max.pres.sfc',\n",
    "#     'max.uwnd.10m',\n",
    "#     'max.vwnd.10m',\n",
    "#     'max.vis',\n",
    "#     'min.air.2m',\n",
    "#     'min.apcp',\n",
    "#     'min.rhum.2m',\n",
    "#     'min.dpt.2m',\n",
    "#     'min.pres.sfc',\n",
    "#     'min.uwnd.10m',\n",
    "#     'min.vwnd.10m',\n",
    "#     'min.vis',\n",
    "#     'total_fires'\n",
    "# ]\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    data_functions.matthews_correlation\n",
    "]\n",
    "\n",
    "dim_learning_rate = Real(\n",
    "    low=1e-5, \n",
    "    high=1e-1, \n",
    "    prior='log-uniform',\n",
    "    name='learning_rate'\n",
    ")\n",
    "\n",
    "dim_hidden_layers = Integer(\n",
    "    low=5,\n",
    "    high=1000, \n",
    "    name='hidden_layers'\n",
    ")\n",
    "\n",
    "dim_neurons_per_layer = Integer(\n",
    "    low=10, \n",
    "    high=500, \n",
    "    name='neurons_per_layer'\n",
    ")\n",
    "\n",
    "dim_dropout_rate = Real(\n",
    "    low=0.01, \n",
    "    high=0.5, \n",
    "    name='dropout_rate'\n",
    ")\n",
    "\n",
    "dim_l2_lambda = Real(\n",
    "    low=0.001, \n",
    "    high=0.1, \n",
    "    name='l2_lambda'\n",
    ")\n",
    "\n",
    "dim_class_0_weight = Real(\n",
    "    low=0.1,\n",
    "    high=5,\n",
    "    name='class_0_weight'\n",
    ")\n",
    "\n",
    "dim_class_1_weight = Real(\n",
    "    low=10,\n",
    "    high=15,\n",
    "    name=\"class_1_weight\"\n",
    ")\n",
    "\n",
    "dimensions = [\n",
    "    dim_learning_rate,\n",
    "    dim_hidden_layers,\n",
    "    dim_neurons_per_layer,\n",
    "    dim_dropout_rate,\n",
    "    dim_l2_lambda,\n",
    "    dim_class_0_weight,\n",
    "    dim_class_1_weight\n",
    "]\n",
    "\n",
    "default_parameters = [0.001, 10, 50, 0.25, 0.05, 0.5, 12]\n",
    "\n",
    "#path_best_model = '../trained_models/best_skopt_MLP.keras'\n",
    "best_matthews_correlation = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_parquet(data_file)\n",
    "validation_data = pd.read_parquet(validation_file)\n",
    "testing_data = pd.read_parquet(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out columns of intrest\n",
    "# training_data = data[features]\n",
    "# validation_data = validation[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode month\n",
    "\n",
    "months = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'Septermber',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December'\n",
    "]\n",
    "\n",
    "\n",
    "# onehot encode month\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# extract month from date column and convert to int\n",
    "month = np.array(pd.DatetimeIndex(training_data['date']).month).reshape(-1, 1)\n",
    "month = month.astype('int32')\n",
    "\n",
    "# onehot encode\n",
    "onehot_month = onehot_encoder.fit_transform(month).astype('int32')\n",
    "\n",
    "# convert one hot encoded months to dataframe\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns = months)\n",
    "\n",
    "# reset indexes, set dtypes and concatenate one hot encoded months \n",
    "# back to orignal dataframe\n",
    "onehot_month_df.reset_index(drop = True, inplace = True)\n",
    "onehot_month_df = onehot_month_df.astype('int32')\n",
    "training_data.reset_index(drop = True, inplace = True)\n",
    "training_data = pd.concat([training_data, onehot_month_df], axis = 1)\n",
    "training_data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# extract month from date column and convert to int\n",
    "month = np.array(pd.DatetimeIndex(validation_data['date']).month).reshape(-1, 1)\n",
    "month = month.astype('int32')\n",
    "\n",
    "# onehot encode\n",
    "onehot_month = onehot_encoder.fit_transform(month).astype('int32')\n",
    "\n",
    "# convert one hot encoded months to dataframe\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns = months)\n",
    "\n",
    "# reset indexes, set dtypes and concatenate one hot encoded months \n",
    "# back to orignal dataframe\n",
    "onehot_month_df.reset_index(drop = True, inplace = True)\n",
    "onehot_month_df = onehot_month_df.astype('int32')\n",
    "validation_data.reset_index(drop = True, inplace = True)\n",
    "validation_data = pd.concat([validation_data, onehot_month_df], axis = 1)\n",
    "validation_data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# extract month from date column and convert to int\n",
    "month = np.array(pd.DatetimeIndex(testing_data['date']).month).reshape(-1, 1)\n",
    "month = month.astype('int32')\n",
    "\n",
    "# onehot encode\n",
    "onehot_month = onehot_encoder.fit_transform(month).astype('int32')\n",
    "\n",
    "# convert one hot encoded months to dataframe\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns = months)\n",
    "\n",
    "# reset indexes, set dtypes and concatenate one hot encoded months \n",
    "# back to orignal dataframe\n",
    "onehot_month_df.reset_index(drop = True, inplace = True)\n",
    "onehot_month_df = onehot_month_df.astype('int32')\n",
    "testing_data.reset_index(drop = True, inplace = True)\n",
    "testing_data = pd.concat([testing_data, onehot_month_df], axis = 1)\n",
    "testing_data.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 449258 entries, 0 to 449257\n",
      "Data columns (total 46 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   lat               449258 non-null  float32\n",
      " 1   lon               449258 non-null  float32\n",
      " 2   month             449258 non-null  int64  \n",
      " 3   mean_air_2m       449258 non-null  float32\n",
      " 4   mean_apcp         449258 non-null  float32\n",
      " 5   mean_rhum_2m      449258 non-null  float32\n",
      " 6   mean_dpt_2m       449258 non-null  float32\n",
      " 7   mean_pres_sfc     449258 non-null  float32\n",
      " 8   mean_uwnd_10m     449258 non-null  float32\n",
      " 9   mean_vwnd_10m     449258 non-null  float32\n",
      " 10  mean_vis          449258 non-null  float32\n",
      " 11  mean_cloud_cover  449258 non-null  float32\n",
      " 12  max_air_2m        449258 non-null  float32\n",
      " 13  max_apcp          449258 non-null  float32\n",
      " 14  max_rhum_2m       449258 non-null  float32\n",
      " 15  max_dpt_2m        449258 non-null  float32\n",
      " 16  max_pres_sfc      449258 non-null  float32\n",
      " 17  max_uwnd_10m      449258 non-null  float32\n",
      " 18  max_vwnd_10m      449258 non-null  float32\n",
      " 19  max_vis           449258 non-null  float32\n",
      " 20  max_cloud_cover   449258 non-null  float32\n",
      " 21  min_air_2m        449258 non-null  float32\n",
      " 22  min_apcp          449258 non-null  float32\n",
      " 23  min_rhum_2m       449258 non-null  float32\n",
      " 24  min_dpt_2m        449258 non-null  float32\n",
      " 25  min_pres_sfc      449258 non-null  float32\n",
      " 26  min_uwnd_10m      449258 non-null  float32\n",
      " 27  min_vwnd_10m      449258 non-null  float32\n",
      " 28  min_vis           449258 non-null  float32\n",
      " 29  min_cloud_cover   449258 non-null  float32\n",
      " 30  ignition          449258 non-null  float32\n",
      " 31  total_fires       449258 non-null  float32\n",
      " 32  crain             449258 non-null  float32\n",
      " 33  veg               449258 non-null  float32\n",
      " 34  January           449258 non-null  int32  \n",
      " 35  February          449258 non-null  int32  \n",
      " 36  March             449258 non-null  int32  \n",
      " 37  April             449258 non-null  int32  \n",
      " 38  May               449258 non-null  int32  \n",
      " 39  June              449258 non-null  int32  \n",
      " 40  July              449258 non-null  int32  \n",
      " 41  August            449258 non-null  int32  \n",
      " 42  Septermber        449258 non-null  int32  \n",
      " 43  October           449258 non-null  int32  \n",
      " 44  November          449258 non-null  int32  \n",
      " 45  December          449258 non-null  int32  \n",
      "dtypes: float32(33), int32(12), int64(1)\n",
      "memory usage: 80.5 MB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 449257 entries, 0 to 449256\n",
      "Data columns (total 46 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   lat               449257 non-null  float32\n",
      " 1   lon               449257 non-null  float32\n",
      " 2   month             449257 non-null  int64  \n",
      " 3   mean_air_2m       449257 non-null  float32\n",
      " 4   mean_apcp         449257 non-null  float32\n",
      " 5   mean_rhum_2m      449257 non-null  float32\n",
      " 6   mean_dpt_2m       449257 non-null  float32\n",
      " 7   mean_pres_sfc     449257 non-null  float32\n",
      " 8   mean_uwnd_10m     449257 non-null  float32\n",
      " 9   mean_vwnd_10m     449257 non-null  float32\n",
      " 10  mean_vis          449257 non-null  float32\n",
      " 11  mean_cloud_cover  449257 non-null  float32\n",
      " 12  max_air_2m        449257 non-null  float32\n",
      " 13  max_apcp          449257 non-null  float32\n",
      " 14  max_rhum_2m       449257 non-null  float32\n",
      " 15  max_dpt_2m        449257 non-null  float32\n",
      " 16  max_pres_sfc      449257 non-null  float32\n",
      " 17  max_uwnd_10m      449257 non-null  float32\n",
      " 18  max_vwnd_10m      449257 non-null  float32\n",
      " 19  max_vis           449257 non-null  float32\n",
      " 20  max_cloud_cover   449257 non-null  float32\n",
      " 21  min_air_2m        449257 non-null  float32\n",
      " 22  min_apcp          449257 non-null  float32\n",
      " 23  min_rhum_2m       449257 non-null  float32\n",
      " 24  min_dpt_2m        449257 non-null  float32\n",
      " 25  min_pres_sfc      449257 non-null  float32\n",
      " 26  min_uwnd_10m      449257 non-null  float32\n",
      " 27  min_vwnd_10m      449257 non-null  float32\n",
      " 28  min_vis           449257 non-null  float32\n",
      " 29  min_cloud_cover   449257 non-null  float32\n",
      " 30  ignition          449257 non-null  float32\n",
      " 31  total_fires       449257 non-null  float32\n",
      " 32  crain             449257 non-null  float32\n",
      " 33  veg               449257 non-null  float32\n",
      " 34  January           449257 non-null  int32  \n",
      " 35  February          449257 non-null  int32  \n",
      " 36  March             449257 non-null  int32  \n",
      " 37  April             449257 non-null  int32  \n",
      " 38  May               449257 non-null  int32  \n",
      " 39  June              449257 non-null  int32  \n",
      " 40  July              449257 non-null  int32  \n",
      " 41  August            449257 non-null  int32  \n",
      " 42  Septermber        449257 non-null  int32  \n",
      " 43  October           449257 non-null  int32  \n",
      " 44  November          449257 non-null  int32  \n",
      " 45  December          449257 non-null  int32  \n",
      "dtypes: float32(33), int32(12), int64(1)\n",
      "memory usage: 80.5 MB\n"
     ]
    }
   ],
   "source": [
    "validation_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(training_data.pop('ignition'))\n",
    "val_labels = np.array(validation_data.pop('ignition'))\n",
    "testing_labels = np.array(testing_data.pop('ignition'))\n",
    "\n",
    "train_features = np.array(training_data)\n",
    "val_features = np.array(validation_data)\n",
    "testing_features = np.array(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (449258,)\n",
      "Validation labels shape: (449257,)\n",
      "Testing labels shape: (449258,)\n",
      "Training features shape: (449258, 45)\n",
      "Validation features shape: (449257, 45)\n",
      "Testing features shape: (449258, 45)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "testing_features = scaler.transform(testing_features)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Testing labels shape:', testing_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Testing features shape:', testing_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignition_count = sum(train_labels)\n",
    "no_ignition_count = len(train_labels) - ignition_count\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 10000\n",
    "STEPS_PER_EPOCH = (len(train_features) * 1) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(val_features) * 1) // BATCH_SIZE\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_matthews_correlation', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    output_bias,\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda\n",
    "):    \n",
    "    # Define the keras model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add input layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        neurons_per_layer, \n",
    "        activation = 'relu', \n",
    "        input_dim = train_features.shape[-1],\n",
    "    ))\n",
    "\n",
    "    # Add fully connected hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(keras.layers.Dense(\n",
    "            neurons_per_layer,\n",
    "            bias_initializer=keras.initializers.VarianceScaling(\n",
    "                scale=1.0,\n",
    "                mode='fan_in', \n",
    "                distribution='normal', \n",
    "                seed=None\n",
    "            ),\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "            activation = 'relu')\n",
    "        )\n",
    "    \n",
    "    # Add dropout layer\n",
    "    model.add(keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        1, \n",
    "        activation = 'sigmoid', \n",
    "        bias_initializer = output_bias\n",
    "    ))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "#run_num = 1\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "    \n",
    "#     # Print the hyper-parameters.\n",
    "#     print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "#     print('hidden layers:', hidden_layers)\n",
    "#     print('neurons per layer:', neurons_per_layer)\n",
    "#     print('dropout rate: {}'.format(np.round(dropout_rate,2)))\n",
    "#     print('l2 lambda: {0:.1e}'.format(l2_lambda))\n",
    "#     print('class weight: {}, {}'.format(np.round(class_weight[0],1), np.round(class_weight[1],2)))\n",
    "#     print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_model(\n",
    "        output_bias,\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_layers = hidden_layers,\n",
    "        neurons_per_layer = neurons_per_layer,\n",
    "        dropout_rate = dropout_rate,\n",
    "        l2_lambda = l2_lambda\n",
    "    )\n",
    "    \n",
    "#     model.summary()\n",
    "#     print()\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "#     log_dir = log_dir_name(\n",
    "#         learning_rate,\n",
    "#         hidden_layers,\n",
    "#         neurons_per_layer,\n",
    "#         dropout_rate,\n",
    "#         l2_lambda,\n",
    "#         class_0_weight,\n",
    "#         class_1_weight\n",
    "#     )\n",
    "    \n",
    "    # Create a callback-function for Keras which will be\n",
    "    # run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    # Note that there are complications when histogram_freq=1.\n",
    "    # It might give strange errors and it also does not properly\n",
    "    # support Keras data-generators for the validation-set.\n",
    "#     callback_log = TensorBoard(\n",
    "#         log_dir=log_dir,\n",
    "#         histogram_freq=0,\n",
    "#         write_graph=True,\n",
    "#         write_grads=False,\n",
    "#         write_images=False\n",
    "#     )\n",
    "   \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data=(val_features, val_labels),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        class_weight=class_weight,\n",
    "        workers=8,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Get MCC for training run\n",
    "    matthews_correlation = history.history['val_matthews_correlation'][-1]\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_matthews_correlation\n",
    "\n",
    "    # If the MCC of the saved model is improved ...\n",
    "    if matthews_correlation > best_matthews_correlation:\n",
    "        # Save the new model to harddisk.\n",
    "        #model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_matthews_correlation = matthews_correlation\n",
    "        \n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    #run_num += 1\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST MCC, we need to \n",
    "    # negate this number so it can be minimized.\n",
    "    return -matthews_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(\n",
    "    func=fitness,\n",
    "    dimensions=dimensions,\n",
    "    acq_func='EI', # Expected Improvement.\n",
    "    n_calls=100,\n",
    "    x0=default_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = [\n",
    "    'learning_rate',\n",
    "    'hidden_layers',\n",
    "    'neurons_per_layer',\n",
    "    'dropout_rate',\n",
    "    'l2_lambda',\n",
    "    'class_0_weight',\n",
    "    'class_1_weight'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_objective(result=search_result, dimension_names=dim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = search_result.space\n",
    "winning_hyperparams = space.point_to_dict(search_result.x)\n",
    "winning_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.001, 2, 30, 0.5, 0.1, 0.5, 15 defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = winning_hyperparams['learning_rate']\n",
    "#learning_rate = 0.001\n",
    "hidden_layers = winning_hyperparams['hidden_layers']\n",
    "units_per_layer = winning_hyperparams['neurons_per_layer']\n",
    "dropout_rate = winning_hyperparams['dropout_rate']\n",
    "l2_lambda = winning_hyperparams['l2_lambda']\n",
    "#l2_lambda = 0.05\n",
    "class_0_weight = winning_hyperparams['class_0_weight']\n",
    "#class_0_weight = 0.6\n",
    "class_1_weight = winning_hyperparams['class_1_weight']\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "    \n",
    "class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 10000\n",
    "STEPS_PER_EPOCH = (len(training_data) * 1) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(validation_data) * 1) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Dense(\n",
    "    units_per_layer, \n",
    "    activation = 'relu', \n",
    "    input_dim = train_features.shape[-1],\n",
    "))\n",
    "\n",
    "# Add fully connected hidden layers\n",
    "for i in range(hidden_layers):\n",
    "    model.add(keras.layers.Dense(\n",
    "        units_per_layer,\n",
    "        bias_initializer=keras.initializers.VarianceScaling(\n",
    "            scale=1.0,\n",
    "            mode='fan_in', \n",
    "            distribution='normal', \n",
    "            seed=None\n",
    "        ),\n",
    "        kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "        activation = 'relu')\n",
    "    )\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(\n",
    "    1, \n",
    "    activation = 'sigmoid', \n",
    "    bias_initializer = output_bias\n",
    "))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    class_weight=class_weight,\n",
    "    workers=8,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "#model.save('../trained_models/best_MLP.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names =  ['loss', 'auc', 'tp', 'fp', 'tn', 'fn']\n",
    "filename = '../project_info/figures/deep_neural_net_learning_curves.png'\n",
    "plotting_functions.plot_metrics(history, metric_names, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_features)\n",
    "validation_predictions = model.predict(val_features)\n",
    "testing_predictions = model.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(\n",
    "    testing_features,\n",
    "    testing_labels, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(name, ': ', value)\n",
    "    \n",
    "filename = '../project_info/figures/deep_neural_net_confusion_matrices.png'\n",
    "plotting_functions.plot_cm(train_labels, train_predictions, testing_labels, testing_predictions, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC for all three datasets\n",
    "plotting_functions.plot_roc(\"Train\", train_labels, train_predictions, color='darkgray')\n",
    "plotting_functions.plot_roc(\"Validation\", val_labels, validation_predictions, color='goldenrod')\n",
    "plotting_functions.plot_roc(\"Test\", testing_labels, testing_predictions, color='darkblue')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('../project_info/figures/deep_neural_net_ROC.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_parquet(data_file)\n",
    "testing_data = pd.read_parquet(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode month\n",
    "\n",
    "months = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'Septermber',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December'\n",
    "]\n",
    "\n",
    "\n",
    "# onehot encode month\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# extract month from date column and convert to int\n",
    "month = np.array(pd.DatetimeIndex(training_data['date']).month).reshape(-1, 1)\n",
    "month = month.astype('int32')\n",
    "\n",
    "# onehot encode\n",
    "onehot_month = onehot_encoder.fit_transform(month).astype('int32')\n",
    "\n",
    "# convert one hot encoded months to dataframe\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns = months)\n",
    "\n",
    "# reset indexes, set dtypes and concatenate one hot encoded months \n",
    "# back to orignal dataframe\n",
    "onehot_month_df.reset_index(drop = True, inplace = True)\n",
    "onehot_month_df = onehot_month_df.astype('int32')\n",
    "training_data.reset_index(drop = True, inplace = True)\n",
    "training_data = pd.concat([training_data, onehot_month_df], axis = 1)\n",
    "training_data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# extract month from date column and convert to int\n",
    "month = np.array(pd.DatetimeIndex(testing_data['date']).month).reshape(-1, 1)\n",
    "month = month.astype('int32')\n",
    "\n",
    "# onehot encode\n",
    "onehot_month = onehot_encoder.fit_transform(month).astype('int32')\n",
    "\n",
    "# convert one hot encoded months to dataframe\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns = months)\n",
    "\n",
    "# reset indexes, set dtypes and concatenate one hot encoded months \n",
    "# back to orignal dataframe\n",
    "onehot_month_df.reset_index(drop = True, inplace = True)\n",
    "onehot_month_df = onehot_month_df.astype('int32')\n",
    "testing_data.reset_index(drop = True, inplace = True)\n",
    "testing_data = pd.concat([testing_data, onehot_month_df], axis = 1)\n",
    "testing_data.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one spatial bin with fires\n",
    "training_data = training_data[(training_data['lat'] == 39.19109) & (training_data['lon'] == -120.2230)]\n",
    "testing_data = testing_data[(testing_data['lat'] == 38.90858) & (testing_data['lon'] == -120.1582)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and label as numpy arrays\n",
    "train_labels = np.array(training_data.pop('ignition'))\n",
    "train_features = np.array(training_data)\n",
    "\n",
    "testing_labels = np.array(testing_data.pop('ignition'))\n",
    "testing_features = np.array(testing_data)\n",
    "\n",
    "# Scale\n",
    "train_features = scaler.transform(train_features)\n",
    "testing_features = scaler.transform(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "train_predictions = model.predict(train_features)\n",
    "test_predictions = model.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplots(1,4,figsize=(14,4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(155,176)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(175,195)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(195,215)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(train_labels)), \n",
    "    train_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(1,200)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../project_info/figures/deep_neural_net_training_predictions.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,4,figsize=(14,4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    testing_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    test_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(155,176)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    testing_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    test_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(175,195)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    testing_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    test_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(195,215)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    testing_labels,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(testing_labels)), \n",
    "    test_predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(1,200)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../project_info/figures/deep_neural_net_test_predictions.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
